{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data analysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import re\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#import xgboost as xgb\n",
    "\n",
    "\n",
    "# Evaluating and tuning the models\n",
    "from sklearn.model_selection import train_test_split , StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Acquire data\n",
    "train_X = pd.read_csv('train_X.csv', sep=',')\n",
    "train_Y = pd.read_csv('train_Y.csv', sep=';')\n",
    "test_X = pd.read_csv('test_X.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical summaries and first observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the data\n",
    "train_X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train_X and train_Y\n",
    "train_df = pd.merge(train_X, train_Y, on='ID')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID feature is useless, we remove it from train set\n",
    "train_df = train_df.drop(['ID'], axis=1)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a list containing both train and test set to apply modifications on both sets in an iterative way\n",
    "combine = [train_df, test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "print('_'*50,'\\n')\n",
    "test_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including only string columns\n",
    "train_df.describe(include=[np.object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count number of null elements in a dataframe column\n",
    "def count_null(col):\n",
    "    return sum(col.isnull())\n",
    "\n",
    "# Describe a dataframe column\n",
    "def describe(col):\n",
    "    description = col.value_counts()\n",
    "    description['NULL']=count_null(col)\n",
    "    print(description, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of each column\n",
    "for col in train_df.columns.values:\n",
    "    describe(train_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of claims\n",
    "claims = train_df.CLAIM_TYPE.value_counts()\n",
    "claims = round(claims.div(claims.sum()/100),1)\n",
    "claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Especially we can note that:\n",
    "\n",
    "Most of the variables are Categorical:\n",
    "* Nominal variables : \n",
    "    SHIPPING_MODE (11 levels)\n",
    "    BUYER_DEPARTMENT (100 levels)\n",
    "    BUYING_DATE (10 levels)\n",
    "    SELLER_COUNTRY (39 levels)\n",
    "    SELLER_DEPARTMENT (98 levels)\n",
    "    PRODUCT_TYPE (137 levels)\n",
    "    CLAIM_TYPE (8 levels)\n",
    "* Dichotomous variables :\n",
    "    WARRANTIES_FLG\n",
    "    CARD_PAYMENT\n",
    "    COUPON_PAYMENT\n",
    "    RSP_PAYMENT\n",
    "    WALLET_PAYMENT\n",
    "* Ordinal variables :\n",
    "    SHIPPING_PRICE (5 levels)\n",
    "    WARRANTIES_PRICE (5 levels)\n",
    "    PRICECLUB_STATUS (5 levels)\n",
    "    PURCHASE_COUNT (6 levels)\n",
    "    SELLER_SCORE_COUNT (5 levels)\n",
    "    ITEM_PRICE (8 levels)\n",
    "    \n",
    "* Quantitative variables : \n",
    "    REGISTRATION_DATE (17 distinct values)\n",
    "    BUYER_BIRTHDAY_DATE (107 distincts values)\n",
    "    SELLER_SCORE_AVERAGE (31 distincts values)\n",
    "    \n",
    "Quantitative variables could also be considered as Categorical\n",
    "\n",
    "Some variables have many missing values:\n",
    "* WARRANTIES_PRICE : 96603 / 96.6% missing values\n",
    "* SHIPPING_PRICE : 67610 / 67.6% missing values\n",
    "* BUYER_BIRTHDAY_DATE : 5836 / 5.8% missing values\n",
    "\n",
    "Dichotomous classes are not well balanced (thus the use of AUC weighted metric):\n",
    "* WARRANTIES_FLG : 96603 / 96.6% zeros\n",
    "* COUPON_PAYMENT : 94271 / 94.3% zeros\n",
    "* CARD_PAYMENT : 10407 / 89,6% ones\n",
    "* WALLET_PAYMENT : 87045 / 87.0% zeros\n",
    "* RSP_PAYMENT : 82942 / 82.9% zeros\n",
    "* CLAIM_TYPE : 49977 / 50.0% '-' (no claim)\n",
    "\n",
    "Some categorical variables have a high cardinality:\n",
    "* PRODUCT_TYPE : 137 distinct values\n",
    "* BUYER_DEPARTMENT 100 distinct values\n",
    "* SELLER_DEPARTMENT 98 distinct values\n",
    "* SELLER_COUNTRY 39 distinct values\n",
    "\n",
    "Dataset has 3238 duplicate rows: Some users may appear in several rows (cf introduction video).\n",
    "Some numerical values are discretized for anonymisation puropose but it might be possible to identify individuals.\n",
    "\n",
    "Features are quite heterogeneous and complex.\n",
    "Thus it could be interesting to find new socio-demographic features based on the current ones to feed our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation:\n",
    "#### - Categorical variables need to be transformed to numeric variables\n",
    "#### - Fill missing values in variables\n",
    "#### - Creation of new variables /  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of claims for each level of categorical variable col, sorted by level importance\n",
    "def claim_percentage_crosstab(col):\n",
    "    df = pd.crosstab(train_df[col], train_df.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    return round(df.div(df.sum(axis=1)/100, axis=0),1)\n",
    "\n",
    "# Mapping of a categorical variable col give a dictionnary mappping\n",
    "def categorical_mapping(col, mapping):\n",
    "    for dataset in combine:\n",
    "        dataset[col] = dataset[col].fillna(0).map(mapping).astype(int)\n",
    "\n",
    "# Turn a categorical variable into dummy variables\n",
    "def categorical_to_dummy(col):\n",
    "    for dataset in combine:\n",
    "        dummy = pd.get_dummies(dataset[col], prefix=col)\n",
    "        for column in dummy.columns.values:\n",
    "            dataset[column] = dummy[column]\n",
    "        del dataset[col]\n",
    "\n",
    "# Plot correlation map given a dataframe df\n",
    "def plot_correlation_map(df):\n",
    "    corr = df.corr()\n",
    "    _, ax = plt.subplots(figsize=(12, 10))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9}, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = {'fontsize':12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A heat map of correlation may give us a understanding of which variables are important\n",
    "plot_correlation_map(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHIPPING_MODE and SHIPPING_PRICE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given SHIPPING_MODE\n",
    "claim_percentage_crosstab('SHIPPING_MODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given SHIPPING_PRICE\n",
    "claim_percentage_crosstab('SHIPPING_PRICE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link between SHIPPING_PRICE and SHIPPING_MODE\n",
    "pd.crosstab(train_df.SHIPPING_MODE, train_df.SHIPPING_PRICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* The delivery ways which are not popular have in average more claims than others.\n",
    "Especially for PICKUP with 38% of WITHDRAWAL claim.\n",
    "\n",
    "* MONDIAL_RELAY_PREPAYE (54%), SUIVI (54%) and NORMAL (53%) have the greater percentage of regular deliveries (without claim), while RECOMMANDE (37%), PICKUP (26%) and Kiala (0%) have the greater percentage of claims.\n",
    "\n",
    "* NORMAL delivery has the highest percentage of NOT_RECEIVED claims (16%) but the lowest percentage of WITHDRAWAL (4%) and UNDEFINED (2%) claims.\n",
    "\n",
    "* SUIVI has the lowest percentage of NOT_RECEIVED mention among popular delivery ways\n",
    "\n",
    "* CHRONOPOST has the highest percentage of DAMAGED claim\n",
    "\n",
    "* SO_POINT_RELAIS, MONDIAL_RELAY and SO_RECOMMANDE has typically the same percentage of claims\n",
    "\n",
    "\n",
    "* Paradoxically, claims percentage grows with the price of the delivery.\n",
    "Especially : UNDEFINED and WITHDRAWAL claims are more likely to happen with a high price.\n",
    "\n",
    "\n",
    "* Their is a clear correlation between SHIPPING_MODE and SHIPPING_PRICE.\n",
    "For example SHIPPING_PRICE over 20 are more likely to be RECOMMANDE.\n",
    "However the prices can vary for a given delivery way and the SHIPPING_PRICE must be kept into our model.\n",
    "\n",
    "* As SHIPPING_MODE is not an ordinal variable we need to turn it into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turning SHIPPING_MODE into dummy variables:\n",
    "categorical_to_dummy('SHIPPING_MODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping SHIPPING_PRICE:\n",
    "# Considering that Null values correspond to the case when shipping is free\n",
    "SHIPPING_PRICE_mapping = {\"<1\": 1, \"1<5\": 2, \"5<10\": 3, \"10<20\": 4, \">20\": 5, 0.0:0} \n",
    "categorical_mapping('SHIPPING_PRICE', SHIPPING_PRICE_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARRANTIES_FLG and WARRANTIES_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given WARRANTIES_FLG\n",
    "claim_percentage_crosstab('WARRANTIES_FLG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given WARRANTIES_PRICE\n",
    "claim_percentage_crosstab('WARRANTIES_PRICE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* People having a warranty are slightly more likely to formulate a claim\n",
    "Especially : WITHDRAWAL happen more frequently when a Warranty is taken.\n",
    "This make sense since withdrawal possiblity can be included in a warranty\n",
    "\n",
    "* Their is no clear link between warranties prices and claims\n",
    "\n",
    "* As the number of null values for WARRANTIES_PRICE is equal to the number of False values for WARRANTIES_FLG, null values exactly correspond to the absence of warranty and can be put at 0 during mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping WARRANTIES_PRICE :\n",
    "WARRANTIES_PRICE_mapping = {\"<5\": 1, \"5<20\": 2, \"20<50\": 3, \"50<100\": 4, \"100<500\": 5, 0.0:0}\n",
    "categorical_mapping('WARRANTIES_PRICE', WARRANTIES_PRICE_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping WARRANTIES_FLG :\n",
    "WARRANTIES_FLG_mapping = {True: 1, False: 0}\n",
    "categorical_mapping('WARRANTIES_FLG', WARRANTIES_FLG_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRICECLUB_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given PRICECLUB_STATUS\n",
    "claim_percentage_crosstab('PRICECLUB_STATUS').reindex(['UNSUBSCRIBED', 'REGULAR', 'PLATINUM', 'SILVER', 'GOLD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* PRICECLUB_STATUS are linked to a number of accumulated points won when doing actions as:\n",
    "selling products, refering a friend, using the prime minister application ...\n",
    "With these points the customer can occasionally benefit from free purchases and gifts\n",
    "\n",
    "* There is no clear link between the PRICECLUB_STATUS and claims.\n",
    "\n",
    "* As UNSUBSCRIBED level represent over 60% of PRICECLUB_STATUS values, null values can be put at the same value than UNSUBSCRIBED during the mapping.\n",
    "\n",
    "* PRICECLUB_STATUS can be considered as an ordinal variable since their is a rank between the types of status:\n",
    "UNSUBSCRIBED<REGULAR<PLATINUM<SILVER<GOLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRICECLUB_STATUS_mapping = {\"UNSUBSCRIBED\": 0, \"REGULAR\": 1, \"PLATINUM\": 2, \"SILVER\": 3, \"GOLD\": 4, 0.0:0}\n",
    "categorical_mapping('PRICECLUB_STATUS', PRICECLUB_STATUS_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGISTRATION_DATE and PURCHASE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given REGISTRATION_DATE\n",
    "new_index = [i for i in range(2001,2018)]\n",
    "df = claim_percentage_crosstab('REGISTRATION_DATE').reindex(new_index)\n",
    "plot = df.plot(figsize=(10, 6));\n",
    "plot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given PURCHASE_COUNT\n",
    "new_index = ['<5','5<20','20<50','50<100','100<500','>500']\n",
    "claim_percentage_crosstab('PURCHASE_COUNT').reindex(new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link between PURCHASE_COUNT and REGISTRATION_DATE\n",
    "pd.crosstab(train_df.REGISTRATION_DATE, train_df.PURCHASE_COUNT).reindex(columns = new_index).plot(figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obervations:\n",
    "\n",
    "* The different percentage for each type of claim given the date of registration are quite constant.\n",
    "However, recent users tend to complain more often than old users as the level '-' is slightly decreasing over the year of registration\n",
    "\n",
    "* It is noticeable that buyers with experience are less likely to have claim type within WITHDRAWAL and UNDEFINED.\n",
    "Indeed a buyer with a great amount of commands is more susceptible to withdraw their command as they are accustomed to buying online.\n",
    "However buyers with experience are also less suceptible to claim for damage or non reception of their command. This can be explained by the fact that these customers have their habits are may often pass command to seller which they already tried the effectiveness. Also Experience buyers may have good practices to avoid potential inneficient sellers that beginner custumer do not have. This feature will definitely be valuable for our model.\n",
    "\n",
    "* As expected their is a link between these two features as buyers with a little number of commands are more suceptible to be recent users.\n",
    "\n",
    "* Also, their is a clear gap between the most experienced users (<5 items) and other users. For this reason creating a new feature UNEXPERIENCED_USER could be valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping PURCHASE_COUNT :\n",
    "PURCHASE_COUNT_mapping = {'<5':0,'5<20':1,'20<50':2,'50<100':3,'100<500':4,'>500':5}\n",
    "categorical_mapping('PURCHASE_COUNT', PURCHASE_COUNT_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creation of UNEXPERIENCED_BUYER :\n",
    "for dataset in combine:\n",
    "    dataset['UNEXPERIENCED_BUYER'] = np.where(dataset['PURCHASE_COUNT']==0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplification of feature REGISTRATION_DATE, \n",
    "# replaced by BUYER_SENIORITY which corresponds to the number of years the buyer is using the service\n",
    "for dataset in combine:\n",
    "    dataset['BUYER_SENIORITY'] = 2017 - dataset['REGISTRATION_DATE']\n",
    "    del dataset['REGISTRATION_DATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUYER_BIRTHDAY_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYER_BIRTHDAY_DATE\n",
    "new_index = [i for i in range(1902, 1981)]\n",
    "plot = claim_percentage_crosstab('BUYER_BIRTHDAY_DATE').reindex(new_index).dropna()[30:].plot(); # Taking off extreme values\n",
    "plot.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* We can observe a correlation between age and claims as claims tend to increase while age decrease.\n",
    "Especially young people seem more likely to claim NOT_RECEIVED which is not obvious.\n",
    "\n",
    "* Replacing birhtday date by age would be simpler for our model\n",
    "\n",
    "* There are a lot of Null values for this feature but it would not be efficient to use the mode or mean values in this case since levels are well balanced. The first option is to use a dynamic fill mathode as ffill which propagate last valid observation forward to next valid. The second option is to use knn on other features that may be correlated to BUYER_BIRTHDAY_DATE to fill the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simplification of feature BUYER_BIRTHDAY_DATE, replaced by BUYER_AGE\n",
    "for dataset in combine:\n",
    "    dataset['BUYER_AGE'] = 2017 - dataset['BUYER_BIRTHDAY_DATE']\n",
    "    del dataset['BUYER_BIRTHDAY_DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filling missing data with ffill method:\n",
    "for dataset in combine:\n",
    "    dataset['BUYER_AGE'] = dataset['BUYER_AGE'].fillna(method='ffill').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: filling missing data with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUYER_DEPARTMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the model we group buyers localisations by regions instead of departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a mapping dictionnary from departments to regions\n",
    "DEPARTMENT_mapping = {}\n",
    "\n",
    "Bretagne = [22,29,35,56]\n",
    "Normandie = [14, 27, 50, 61, 76]\n",
    "Hauts_De_France = [2,59,60,62,80]\n",
    "Ile_De_France = [77,78,91,95]\n",
    "Paris = [75,92,93,94]\n",
    "Grand_Est =[8,10,51,52,54,55,57,67,68,88]\n",
    "Bourgogne_France_Compte = [21,25,39,58,70,71,89,90]\n",
    "Nouvelle_Aquitaine = [16,17,19,23,24,33,40,47,64,79,86,87]\n",
    "Auvergne_Rhone_Alpes = [1,3,7,15,26,38,42,43,63,69,73,74]\n",
    "Occitanie = [9,11,12,30,31,32,34,46,48,65,66,81,82]\n",
    "Centre_Val_De_Loire = [18,28,36,37,41,45]\n",
    "PACA = [4,5,6,13,83,84]\n",
    "Pays_De_La_Loire = [44,49,53,72,85]\n",
    "Etranger = [-1,20,97] # Putting out of France departments into Foreign list, 20=Corse, 97=DOM/TOM\n",
    "Null = [0,96,98,99,100] # null values (department 97 and 98 do not exist)\n",
    "\n",
    "Regions = [Bretagne,Normandie,Hauts_De_France,Ile_De_France,Paris,Grand_Est,Bourgogne_France_Compte,Nouvelle_Aquitaine,\n",
    "           Auvergne_Rhone_Alpes,Occitanie,Centre_Val_De_Loire,PACA,Pays_De_La_Loire,Etranger,Null]\n",
    "\n",
    "noms_Regions = ['Bretagne','Normandie','Hauts_De_France','Ile_De_France','Paris','Grand_Est','Bourgogne_France_Compte',\n",
    "                'Nouvelle_Aquitaine','Auvergne_Rhone_Alpes','Occitanie','Centre_Val_De_Loire','PACA',\n",
    "                'Pays_De_La_Loire','Etranger',0]\n",
    "\n",
    "for ind, region in enumerate(Regions):\n",
    "    for i in region:\n",
    "        DEPARTMENT_mapping[i]=noms_Regions[ind]\n",
    "\n",
    "#BUYER_DEPARTMENT_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating BUYER_REGION :\n",
    "for dataset in combine:\n",
    "    dataset['BUYER_REGION'] = dataset['BUYER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    del dataset['BUYER_DEPARTMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYER_REGION\n",
    "claim_percentage_crosstab('BUYER_REGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : \n",
    "\n",
    "* Their is no clear correlation between BUYER_REGION and CLAIM_TYPE if buyer live in France.\n",
    "We can only notice that buyer from Ile_De_France region (including Paris) and from PACA are slightly more suceptible to claim NOT_RECEIVED.\n",
    "\n",
    "* However, as expected, claims happen more often abroad, as delivery is more complex.\n",
    "Indeed commands passed abroad are more likely to receive claims NOT_RECEIVED or SELLER_CANCEL_POSTERIORI while on the contrary claims DAMAGED or DIFFERENT happen less frequently (this point is less obvious).\n",
    "\n",
    "* Thus, it seems valuable to add a feature informing if the the command was passed in France or not.\n",
    "\n",
    "* We could also turn BUYER_REGION into dummy variables to keep information about regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creation of BUYER_IS_ABROAD\n",
    "for dataset in combine:\n",
    "    dataset['BUYER_IS_ABROAD'] = np.where(dataset['BUYER_REGION']=='Etranger', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turning BUYER_REGION into dummy_variables\n",
    "#categorical_to_dummy('BUYER_REGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUYING_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping BUYING_DATE into numerical variable: '3/2017' -> 3\n",
    "# Using regular expression to isolate months\n",
    "for dataset in combine:\n",
    "    dataset['BUYING_DATE'] = dataset['BUYING_DATE'].apply( lambda s : int(re.findall('[0-9]*',s)[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYING_DATE\n",
    "claim_percentage_crosstab('BUYING_DATE').reindex([i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of commands for each month\n",
    "train_df.BUYING_DATE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* There is no clear correlation between BUYING_DATE and CLAIM_TYPE\n",
    "\n",
    "* We can still notice that the month where the amount of commands is at the top (January) have the greatest percentage of claims while the month where the amount of commands is tIs he lowest (October) have the lowest percentage of claims.\n",
    "\n",
    "* It is unfortunate that the data for November and December which are around Christmas are not available.\n",
    "These data would have bring valuable information about a period that might be a peak period.\n",
    "\n",
    "* With respect to the previous notes, keeping only 3 levels to separate the particualar months January and October from the rest of the months that have overall the same stats could represent a more valuable feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping BUYING_DATE\n",
    "# 0 for January where claims are at the top (53% claims)\n",
    "# 1 for February to November where claims are around average (around 50%)\n",
    "# 2 for October where claims are at the lowest (42% claims)\n",
    "BUYING_DATE_mapping = {1:0,10:2}\n",
    "for i in range(2,10):\n",
    "    BUYING_DATE_mapping[i]=1\n",
    "categorical_mapping('BUYING_DATE', BUYING_DATE_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELLER_SCORE_COUNT and SELLER_SCORE_AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYING_DATE\n",
    "claim_percentage_crosstab('SELLER_SCORE_COUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given SELLER_SCORE_AVERAGE\n",
    "df=claim_percentage_crosstab('SELLER_SCORE_AVERAGE').reindex([i for i in range(50,40,-1)]) # Taking off extremely low values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['-'].plot();\n",
    "ax.set_ylabel(\"Percentage of delivery with no claim\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link between SELLER_SCORE_COUNT and SELLER_SCORE_AVERAGE\n",
    "pd.crosstab(train_df.SELLER_SCORE_COUNT, train_df.SELLER_SCORE_AVERAGE[train_df.SELLER_SCORE_AVERAGE>40] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* Logically we can observe that the amount of product sold by the seller is growing while the number of claims is decreasing. This is true for all the claim types except one : SELLER_CANCEL_POSTERIORI percentage tends to be higher for the range 100000<1000000 of SELLER_SCORE_COUNT.\n",
    "\n",
    "* The null values of SELLER_SCORE_COUNT will be put at 0 in the mapping as it can correspond to seller without any product sold.\n",
    "\n",
    "* Just as for the last feature it is clear that seller with the highest score are the more reliable and result in a fewer number of claims. Especially seller with a mark of 49 close to the maximum have a number of claims significatively low. Thus it could be useful to add a feature to discriminate these top seller.\n",
    "\n",
    "* However an important point is that the best score possible 50 is reached only by a few individuals (51, while 3994 for 49 and 18006 for 48) and have in average in very bad reliability. We can also notice that these profiles all have sold less than 100 items. Thus, we can assume that some of these profiles are fake and manage somehow to get the maximum score to trick the system and get people trust. It could also be that those profiles have only a few sells hence the maximum score wich is impossible for a great number of sells.\n",
    "Anyway, as a result these profiles will be put in the same level as seller with bad scores.\n",
    "\n",
    "* We can add that without surprise their is an important correlation between SELLER_SCORE_AVERAGE and SELLER_SCORE_COUNT as reliable seller have in majority sold a lot of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping SELLER_SCORE_COUNT\n",
    "SELLER_SCORE_COUNT_mapping = {0.0:0, '<100':1, '100<1000':2, '1000<10000':3, '10000<100000':4, '100000<1000000':5}\n",
    "categorical_mapping('SELLER_SCORE_COUNT', SELLER_SCORE_COUNT_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping SELLER_SCORE_AVERAGE\n",
    "def SELLER_SCORE_AVERAGE_map(score):\n",
    "    if score < 44 or score == 50:\n",
    "        return 0\n",
    "    elif score < 46:\n",
    "        return 1\n",
    "    elif score == 46:\n",
    "        return 2\n",
    "    elif score == 47:\n",
    "        return 3\n",
    "    elif score == 48:\n",
    "        return 4\n",
    "    elif score == 49:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "for dataset in combine:\n",
    "    dataset['SELLER_SCORE_AVERAGE'] = dataset['SELLER_SCORE_AVERAGE'].apply(SELLER_SCORE_AVERAGE_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create VIP_SELLER feature for seller with a score of 49 (label 5)\n",
    "for dataset in combine:\n",
    "    dataset['VIP_SELLER'] = np.where(dataset['SELLER_SCORE_AVERAGE']==5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELLER_COUNTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_percentage_crosstab('SELLER_COUNTRY')[:30] # Taking off country with a few number of deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Most of European countries have correct percentage of delivery.\n",
    "For instance: SWITZERLAND (50.1% no claim), GERMANY (60,9% no claim), BELGIUM (58,2% no claim), LUXEMBOURG (58,5% no claim).\n",
    "\n",
    "* However some European country perform bad like UNITED KINGDOM\tthat have notably high percentage of NOT_RECEIVED claims (19.2%).\n",
    "\n",
    "* It is hard to classify these country in classes as many countries have specificities (very high percentage of WITHDRAWAL claims for ITALY, overall high percentage of no claim for UNITED STATES however very high percentage of NOT_RECEIVED claims etc...)\n",
    "We then need to add as many dummy variables as there are countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turning SELLER_COUNTRY into dummy variables:\n",
    "categorical_to_dummy('SELLER_COUNTRY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELLER_DEPARTMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating SELLER_REGION : (we use the same department mapping as for BUYER_REGION)\n",
    "for dataset in combine:\n",
    "    dataset['SELLER_REGION'] = dataset['SELLER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    del dataset['SELLER_DEPARTMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYER_REGION\n",
    "claim_percentage_crosstab('SELLER_REGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* Unlike with BUYER_REGION where no correlation was noticeable with CLAIM_TYPE, here there are some differences between the places where the seller sends the command: commands sent from Paris (59,9% no claim) is overall more reliable than commands from Pays_De_La_Loire (34.2% no claim) for example.\n",
    "Thus we need to turn BUYER_REGION into dummy variables.\n",
    "\n",
    "* What's more, it could be unteresting to create an additionnal feature corresponding to commands sent and received in the same region (where SELLER_REGION=BUYER_REGION), since these commands may be more reliable in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating SAME_REGION_BUYER_SELLER equal to 1 when Buyer and Seller are from the same region\n",
    "for dataset in combine:\n",
    "    dataset['SAME_REGION_BUYER_SELLER'] = np.where(dataset['SELLER_REGION']==dataset['BUYER_REGION'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turning BUYER_REGION into dummy_variables\n",
    "categorical_to_dummy('SELLER_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete the residual dummy variable linked to null values\n",
    "del train_df['SELLER_REGION_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can delete BUYER_REGION\n",
    "for dataset in combine:\n",
    "    del dataset['BUYER_REGION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRODUCT_TYPE and PRODUCT_FAMILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given PRODUCT_TYPE\n",
    "claim_percentage_crosstab('PRODUCT_TYPE')[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given PRODUCT_FAMILY\n",
    "claim_percentage_crosstab('PRODUCT_FAMILY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* As PRODUCT_FAMILY already condensate most of information about products, we keep only a few label from PRODUCT_TYPE which stand out from the other labels are are not already in PRODUCT_FAMILY (Books in both for example). \n",
    "Among variables we can keep among PRODUCT_TYPE: PLAY CARDS (83.9% no claim), CD (60,5% no claim), COSMETIC (37.4% no claim), CELLPHONE (31,2% no claim).\n",
    "\n",
    "* ELECTRONICS are more likely to lead to DAMAGED claims.\n",
    "Indeed, electronic devices such as Television are more fragile and can be damaged during transport.\n",
    "On the contrary Wine products are unlikely to be damaged, certainly due to the special care and protections set for the transport since Wine are luxury products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turning PRODUCT_FAMILY into dummy_variables\n",
    "categorical_to_dummy('PRODUCT_FAMILY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating dummy variables for PLAY CARDS, CD, COSMETIC and CELLPHONE:\n",
    "for dataset in combine :\n",
    "    dataset['PRODUCT_TYPE_PLAY_CARDS'] = np.where(dataset['PRODUCT_TYPE']=='PLAY CARD', 1, 0)\n",
    "    dataset['PRODUCT_TYPE_CD'] = np.where(dataset['PRODUCT_TYPE']=='CD', 1, 0)\n",
    "    dataset['PRODUCT_TYPE_COSMETIC'] = np.where(dataset['PRODUCT_TYPE']=='COSMETIC', 1, 0)\n",
    "    dataset['PRODUCT_TYPE_CELLPHONE'] = np.where(dataset['PRODUCT_TYPE']=='CELLPHONE', 1, 0)\n",
    "    del dataset['PRODUCT_TYPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITEM_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(train_df['ITEM_PRICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given ITEM_PRICE\n",
    "claim_percentage_crosstab('ITEM_PRICE').reindex(['<10','10<20','20<50','50<100','100<500','500<1000','1000<5000','>5000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Cheap products are less likely to lead to claims.\n",
    "\n",
    "* Pricey products are more likely to lead to WITHDRAWAL/UNDEFINED claims.\n",
    "\n",
    "* Cheap products are more likely to lead to FAKE/NOT_RECEIVED products.\n",
    "\n",
    "* Products in range 100<500 typically lead more to DAMAGED claims in average.\n",
    "This corresponds to the price range of electronics, which are more likely to lead to DAMAGED mentions as we just saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping ITEM_PRICE:\n",
    "ITEM_PRICE_mapping = {\"<10\" : 1, \"10<20\" : 2, \"20<50\" : 3, \"50<100\" : 4, \"100<500\" : 5, \"500<1000\" : 6,\n",
    "                      \"1000<5000\" : 7, \">5000\" : 8} \n",
    "categorical_mapping('ITEM_PRICE', ITEM_PRICE_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create all datasets that are necessary to train, validate and test models\n",
    "train_X_full = train_df.drop('CLAIM_TYPE',1).astype(int)\n",
    "train_Y_full = train_df.CLAIM_TYPE\n",
    "test_X_full = test_X\n",
    "test_X_ID = test_X['ID']\n",
    "test_X = test_X.drop('ID',1).astype(int)\n",
    "\n",
    "train_Y_multiclass = label_binarize(train_Y_full, classes=[\"-\",\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping train_Y:\n",
    "#train_Y_full_mapping = {\"-\" : 0, \"WITHDRAWAL\" : 1, \"DAMAGED\" : 2, \"DIFFERENT\" : 3, \"SELLER_CANCEL_POSTERIORI\" : 4,\n",
    "#                   \"NOT_RECEIVED\" : 5, \"UNDEFINED\" : 6, \"FAKE\" : 7} \n",
    "#train_Y_full = train_Y_full.map(train_Y_full_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X_full, train_Y_multiclass, \n",
    "                                                      train_size=.7, random_state=7)\n",
    "\n",
    "print (train_X_full.shape, train_Y_full.shape, train_X.shape, valid_X.shape, \n",
    "       train_Y.shape , valid_Y.shape , test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_Y = [\"-\",\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\"UNDEFINED\",\"FAKE\"]\n",
    "\n",
    "# Display the confusion matrix\n",
    "def display_cm(model):\n",
    "    model.fit(train_X, train_Y)\n",
    "    pred_Y = model.predict(valid_X)\n",
    "    cm = confusion_matrix(valid_Y, pred_Y)\n",
    "    cm_df = pd.DataFrame(cm, index=index_Y, columns=index_Y)\n",
    "    return cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "random_forest.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_Y = random_forest.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_weighted(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred, average='weighted')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_weighted(valid_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = train_Y.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(valid_Y[:, i], pred_Y[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(valid_Y.ravel(), pred_Y.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "lw=2\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
