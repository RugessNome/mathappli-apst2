{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data analysis \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import re\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#import xgboost as xgb\n",
    "\n",
    "\n",
    "# Evaluating and tuning the models\n",
    "from sklearn.model_selection import train_test_split , StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Acquire data\n",
    "train_X = pd.read_csv('train_X.csv', sep=',')\n",
    "train_Y = pd.read_csv('train_Y.csv', sep=';')\n",
    "test_X = pd.read_csv('test_X.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical summaries and first observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preview the data\n",
    "train_X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge train_X and train_Y\n",
    "train_df = pd.merge(train_X, train_Y, on='ID')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ID feature is useless, we remove it from train set\n",
    "train_df = train_df.drop(['ID'], axis=1)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a list containing both train and test set to apply modifications on both sets in an iterative way\n",
    "combine = [train_df, test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "print('_'*50,'\\n')\n",
    "test_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Including only string columns\n",
    "train_df.describe(include=[np.object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count number of null elements in a dataframe column\n",
    "def count_null(col):\n",
    "    return sum(col.isnull())\n",
    "\n",
    "# Describe a dataframe column\n",
    "def describe(col):\n",
    "    description = col.value_counts()\n",
    "    description['NULL']=count_null(col)\n",
    "    print(description, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Description of each column\n",
    "for col in train_df.columns.values:\n",
    "    describe(train_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of claims\n",
    "claims = train_df.CLAIM_TYPE.value_counts()\n",
    "claims = round(claims.div(claims.sum()/100),1)\n",
    "claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Especially we can note that:\n",
    "\n",
    "Most of the variables are Categorical:\n",
    "* Nominal variables : \n",
    "    SHIPPING_MODE (11 levels)\n",
    "    BUYER_DEPARTMENT (100 levels)\n",
    "    BUYING_DATE (10 levels)\n",
    "    SELLER_COUNTRY (39 levels)\n",
    "    SELLER_DEPARTMENT (98 levels)\n",
    "    PRODUCT_TYPE (137 levels)\n",
    "    CLAIM_TYPE (8 levels)\n",
    "* Dichotomous variables :\n",
    "    WARRANTIES_FLG\n",
    "    CARD_PAYMENT\n",
    "    COUPON_PAYMENT\n",
    "    RSP_PAYMENT\n",
    "    WALLET_PAYMENT\n",
    "* Ordinal variables :\n",
    "    SHIPPING_PRICE (5 levels)\n",
    "    WARRANTIES_PRICE (5 levels)\n",
    "    PRICECLUB_STATUS (5 levels)\n",
    "    PURCHASE_COUNT (6 levels)\n",
    "    SELLER_SCORE_COUNT (5 levels)\n",
    "    ITEM_PRICE (8 levels)\n",
    "    \n",
    "* Quantitative variables : \n",
    "    REGISTRATION_DATE (17 distinct values)\n",
    "    BUYER_BIRTHDAY_DATE (107 distincts values)\n",
    "    SELLER_SCORE_AVERAGE (31 distincts values)\n",
    "    \n",
    "Quantitative variables could also be considered as Categorical\n",
    "\n",
    "Some variables have many missing values:\n",
    "* WARRANTIES_PRICE : 96603 / 96.6% missing values\n",
    "* SHIPPING_PRICE : 67610 / 67.6% missing values\n",
    "* BUYER_BIRTHDAY_DATE : 5836 / 5.8% missing values\n",
    "\n",
    "Dichotomous classes are not well balanced (thus the use of AUC weighted metric):\n",
    "* WARRANTIES_FLG : 96603 / 96.6% zeros\n",
    "* COUPON_PAYMENT : 94271 / 94.3% zeros\n",
    "* CARD_PAYMENT : 10407 / 89,6% ones\n",
    "* WALLET_PAYMENT : 87045 / 87.0% zeros\n",
    "* RSP_PAYMENT : 82942 / 82.9% zeros\n",
    "* CLAIM_TYPE : 49977 / 50.0% '-' (no claim)\n",
    "\n",
    "Some categorical variables have a high cardinality:\n",
    "* PRODUCT_TYPE : 137 distinct values\n",
    "* BUYER_DEPARTMENT 100 distinct values\n",
    "* SELLER_DEPARTMENT 98 distinct values\n",
    "* SELLER_COUNTRY 39 distinct values\n",
    "\n",
    "Dataset has 3238 duplicate rows: Some users may appear in several rows (cf introduction video).\n",
    "Some numerical values are discretized for anonymisation puropose but it might be possible to identify individuals.\n",
    "\n",
    "Features are quite heterogeneous and complex.\n",
    "Thus it could be interesting to find new socio-demographic features based on the current ones to feed our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-depth dataset study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will study the dataset in greater details. This will help creating new variables / features to train the classifiers (feature engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of claims for each level of categorical variable col, sorted by level importance\n",
    "def claim_percentage_crosstab(col):\n",
    "    df = pd.crosstab(train_df[col], train_df.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    return round(df.div(df.sum(axis=1)/100, axis=0),1)\n",
    "\n",
    "# Plot correlation map given a dataframe df\n",
    "# Note: works only on quantitative variables\n",
    "def plot_correlation_map(df):\n",
    "    corr = df.corr()\n",
    "    _, ax = plt.subplots(figsize=(12, 10))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    _ = sns.heatmap(\n",
    "        corr, \n",
    "        cmap = cmap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9}, \n",
    "        ax=ax, \n",
    "        annot = True, \n",
    "        annot_kws = {'fontsize':12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A heat map of correlation may give us a understanding of which variables are important\n",
    "plot_correlation_map(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHIPPING_MODE and SHIPPING_PRICE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given SHIPPING_MODE\n",
    "claim_percentage_crosstab('SHIPPING_MODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given SHIPPING_PRICE\n",
    "claim_percentage_crosstab('SHIPPING_PRICE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Link between SHIPPING_PRICE and SHIPPING_MODE\n",
    "pd.crosstab(train_df.SHIPPING_MODE, train_df.SHIPPING_PRICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* The delivery ways which are not popular have in average more claims than others.\n",
    "Especially for PICKUP with 38% of WITHDRAWAL claim.\n",
    "\n",
    "* MONDIAL_RELAY_PREPAYE (54%), SUIVI (54%) and NORMAL (53%) have the greater percentage of regular deliveries (without claim), while RECOMMANDE (37%), PICKUP (26%) and Kiala (0%) have the greater percentage of claims.\n",
    "\n",
    "* NORMAL delivery has the highest percentage of NOT_RECEIVED claims (16%) but the lowest percentage of WITHDRAWAL (4%) and UNDEFINED (2%) claims.\n",
    "\n",
    "* SUIVI has the lowest percentage of NOT_RECEIVED mention among popular delivery ways\n",
    "\n",
    "* CHRONOPOST has the highest percentage of DAMAGED claim\n",
    "\n",
    "* SO_POINT_RELAIS, MONDIAL_RELAY and SO_RECOMMANDE has typically the same percentage of claims\n",
    "\n",
    "\n",
    "* Paradoxically, claims percentage grows with the price of the delivery.\n",
    "Especially : UNDEFINED and WITHDRAWAL claims are more likely to happen with a high price.\n",
    "\n",
    "\n",
    "* Their is a clear correlation between SHIPPING_MODE and SHIPPING_PRICE.\n",
    "For example SHIPPING_PRICE over 20 are more likely to be RECOMMANDE.\n",
    "However the prices can vary for a given delivery way and the SHIPPING_PRICE must be kept into our model.\n",
    "\n",
    "* As SHIPPING_MODE is not an ordinal variable we need to turn it into dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARRANTIES_FLG and WARRANTIES_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given WARRANTIES_FLG\n",
    "claim_percentage_crosstab('WARRANTIES_FLG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given WARRANTIES_PRICE\n",
    "claim_percentage_crosstab('WARRANTIES_PRICE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* People having a warranty are slightly more likely to formulate a claim\n",
    "Especially : WITHDRAWAL happen more frequently when a Warranty is taken.\n",
    "This make sense since withdrawal possiblity can be included in a warranty\n",
    "\n",
    "* Their is no clear link between warranties prices and claims\n",
    "\n",
    "* As the number of null values for WARRANTIES_PRICE is equal to the number of False values for WARRANTIES_FLG, null values exactly correspond to the absence of warranty and can be put at 0 during mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRICECLUB_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given PRICECLUB_STATUS\n",
    "claim_percentage_crosstab('PRICECLUB_STATUS').reindex(['UNSUBSCRIBED', 'REGULAR', 'PLATINUM', 'SILVER', 'GOLD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* PRICECLUB_STATUS are linked to a number of accumulated points won when doing actions as:\n",
    "selling products, refering a friend, using the prime minister application ...\n",
    "With these points the customer can occasionally benefit from free purchases and gifts\n",
    "\n",
    "* There is no clear link between the PRICECLUB_STATUS and claims.\n",
    "\n",
    "* As UNSUBSCRIBED level represent over 60% of PRICECLUB_STATUS values, null values can be put at the same value than UNSUBSCRIBED during the mapping.\n",
    "\n",
    "* PRICECLUB_STATUS can be considered as an ordinal variable since their is a rank between the types of status:\n",
    "UNSUBSCRIBED<REGULAR<PLATINUM<SILVER<GOLD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGISTRATION_DATE and PURCHASE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given REGISTRATION_DATE\n",
    "new_index = [i for i in range(2001,2018)]\n",
    "df = claim_percentage_crosstab('REGISTRATION_DATE').reindex(new_index)\n",
    "plot = df.plot(figsize=(10, 6));\n",
    "plot.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given PURCHASE_COUNT\n",
    "new_index = ['<5','5<20','20<50','50<100','100<500','>500']\n",
    "claim_percentage_crosstab('PURCHASE_COUNT').reindex(new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Link between PURCHASE_COUNT and REGISTRATION_DATE\n",
    "pd.crosstab(train_df.REGISTRATION_DATE, train_df.PURCHASE_COUNT).reindex(columns = new_index).plot(figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obervations:\n",
    "\n",
    "* The different percentage for each type of claim given the date of registration are quite constant.\n",
    "However, recent users tend to complain more often than old users as the level '-' is slightly decreasing over the year of registration\n",
    "\n",
    "* It is noticeable that buyers with experience are less likely to have claim type within WITHDRAWAL and UNDEFINED.\n",
    "Indeed a buyer with a great amount of commands is more susceptible to withdraw their command as they are accustomed to buying online.\n",
    "However buyers with experience are also less suceptible to claim for damage or non reception of their command. This can be explained by the fact that these customers have their habits are may often pass command to seller which they already tried the effectiveness. Also Experience buyers may have good practices to avoid potential inneficient sellers that beginner custumer do not have. This feature will definitely be valuable for our model.\n",
    "\n",
    "* As expected their is a link between these two features as buyers with a little number of commands are more suceptible to be recent users.\n",
    "\n",
    "* Also, their is a clear gap between the most experienced users (<5 items) and other users. For this reason creating a new feature UNEXPERIENCED_USER could be valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUYER_BIRTHDAY_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYER_BIRTHDAY_DATE\n",
    "new_index = [i for i in range(1902, 1981)]\n",
    "plot = claim_percentage_crosstab('BUYER_BIRTHDAY_DATE').reindex(new_index).dropna()[30:].plot(); # Taking off extreme values\n",
    "plot.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* We can observe a correlation between age and claims as claims tend to increase while age decrease.\n",
    "Especially young people seem more likely to claim NOT_RECEIVED which is not obvious.\n",
    "\n",
    "* Replacing birhtday date by age would be simpler for our model\n",
    "\n",
    "* There are a lot of Null values for this feature but it would not be efficient to use the mode or mean values in this case since levels are well balanced. The first option is to use a dynamic fill mathode as ffill which propagate last valid observation forward to next valid. The second option is to use knn on other features that may be correlated to BUYER_BIRTHDAY_DATE to fill the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code shows that all ages are not well represented.\n",
    "# However, for a range of reasonable ages, we have enough individuals.\n",
    "values = train_df['BUYER_BIRTHDAY_DATE'].value_counts()\n",
    "pd.set_option('display.max_rows', len(values))\n",
    "print(values)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As we can see from the graph, age has an influence on the claims.\n",
    "# However, considering each age might be too expensiveand subject to overfitting\n",
    "# especially on birthdate that are not so represented.\n",
    "# Instead, we can try to study things by grouping people with similar age\n",
    "def group_by_age(min_threshold, max_threshold, step, birthday_series):\n",
    "    from math import ceil\n",
    "    age_series = (2017 - birthday_series).rename('BUYER_AGE')\n",
    "    nb_group = 1 + 1 + int(ceil((max_threshold - min_threshold) / step))\n",
    "    def get_group(x):\n",
    "        if x <= min_threshold:\n",
    "            return 0\n",
    "        elif x > max_threshold:\n",
    "            return nb_group-1\n",
    "        x = x - min_threshold\n",
    "        group = 0\n",
    "        while x > 0:\n",
    "            group = group + 1\n",
    "            x = x - step\n",
    "        return group\n",
    "    return age_series.map(get_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize_claims(series):\n",
    "    def claim_filter(c):\n",
    "        return 0 if c == '-' else 1\n",
    "    return series.map(claim_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(group_by_age(20, 60, 5, train_df.BUYER_BIRTHDAY_DATE), binarize_claims(train_df.CLAIM_TYPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUYER_DEPARTMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code shows that not all departments are well represented\n",
    "values = train_df['BUYER_DEPARTMENT'].value_counts()\n",
    "pd.set_option('display.max_rows', len(values))\n",
    "print(values)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.BUYER_DEPARTMENT, binarize_claims(train_df.CLAIM_TYPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the model we might want to group buyers localisations by regions instead of departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mapping dictionnary from departments to regions\n",
    "DEPARTMENT_mapping = {}\n",
    "\n",
    "Bretagne = [22,29,35,56]\n",
    "Normandie = [14, 27, 50, 61, 76]\n",
    "Hauts_De_France = [2,59,60,62,80]\n",
    "Ile_De_France = [77,78,91,95]\n",
    "Paris = [75,92,93,94]\n",
    "Grand_Est =[8,10,51,52,54,55,57,67,68,88]\n",
    "Bourgogne_France_Compte = [21,25,39,58,70,71,89,90]\n",
    "Nouvelle_Aquitaine = [16,17,19,23,24,33,40,47,64,79,86,87]\n",
    "Auvergne_Rhone_Alpes = [1,3,7,15,26,38,42,43,63,69,73,74]\n",
    "Occitanie = [9,11,12,30,31,32,34,46,48,65,66,81,82]\n",
    "Centre_Val_De_Loire = [18,28,36,37,41,45]\n",
    "PACA = [4,5,6,13,83,84]\n",
    "Pays_De_La_Loire = [44,49,53,72,85]\n",
    "Etranger = [-1,20,97] # Putting out of France departments into Foreign list, 20=Corse, 97=DOM/TOM\n",
    "Null = [0,96,98,99,100] # null values (department 97 and 98 do not exist)\n",
    "\n",
    "Regions = [Bretagne,Normandie,Hauts_De_France,Ile_De_France,Paris,Grand_Est,Bourgogne_France_Compte,Nouvelle_Aquitaine,\n",
    "           Auvergne_Rhone_Alpes,Occitanie,Centre_Val_De_Loire,PACA,Pays_De_La_Loire,Etranger,Null]\n",
    "\n",
    "noms_Regions = ['Bretagne','Normandie','Hauts_De_France','Ile_De_France','Paris','Grand_Est','Bourgogne_France_Compte',\n",
    "                'Nouvelle_Aquitaine','Auvergne_Rhone_Alpes','Occitanie','Centre_Val_De_Loire','PACA',\n",
    "                'Pays_De_La_Loire','Etranger',0]\n",
    "\n",
    "for ind, region in enumerate(Regions):\n",
    "    for i in region:\n",
    "        DEPARTMENT_mapping[i]=noms_Regions[ind]\n",
    "\n",
    "# Creating BUYER_REGION :\n",
    "for dataset in combine:\n",
    "    dataset['BUYER_REGION'] = dataset['BUYER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    del dataset['BUYER_DEPARTMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYER_REGION\n",
    "claim_percentage_crosstab('BUYER_REGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : \n",
    "\n",
    "* Their is no clear correlation between BUYER_REGION and CLAIM_TYPE if buyer live in France.\n",
    "We can only notice that buyer from Ile_De_France region (including Paris) and from PACA are slightly more suceptible to claim NOT_RECEIVED.\n",
    "\n",
    "* However, as expected, claims happen more often abroad, as delivery is more complex.\n",
    "Indeed commands passed abroad are more likely to receive claims NOT_RECEIVED or SELLER_CANCEL_POSTERIORI while on the contrary claims DAMAGED or DIFFERENT happen less frequently (this point is less obvious).\n",
    "\n",
    "* Thus, it seems valuable to add a feature informing if the the command was passed in France or not.\n",
    "\n",
    "* We could also turn BUYER_REGION into dummy variables to keep information about regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BUYING_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping BUYING_DATE into numerical variable: '3/2017' -> 3\n",
    "# Using regular expression to isolate months\n",
    "for dataset in combine:\n",
    "    dataset['BUYING_DATE'] = dataset['BUYING_DATE'].apply( lambda s : int(re.findall('[0-9]*',s)[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYING_DATE\n",
    "claim_percentage_crosstab('BUYING_DATE').reindex([i for i in range(1,11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# frequency of commands for each month\n",
    "train_df.BUYING_DATE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* There is no clear correlation between BUYING_DATE and CLAIM_TYPE\n",
    "\n",
    "* We can still notice that the month where the amount of commands is at the top (January) have the greatest percentage of claims while the month where the amount of commands is tIs he lowest (October) have the lowest percentage of claims.\n",
    "\n",
    "* It is unfortunate that the data for November and December which are around Christmas are not available.\n",
    "These data would have bring valuable information about a period that might be a peak period.\n",
    "\n",
    "* With respect to the previous notes, keeping only 3 levels to separate the particualar months January and October from the rest of the months that have overall the same stats could represent a more valuable feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELLER_SCORE_COUNT and SELLER_SCORE_AVERAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given BUYING_DATE\n",
    "claim_percentage_crosstab('SELLER_SCORE_COUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.SELLER_SCORE_AVERAGE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given SELLER_SCORE_AVERAGE\n",
    "df=claim_percentage_crosstab('SELLER_SCORE_AVERAGE').reindex([i for i in range(50,40,-1)]) # Taking off extremely low values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = df['-'].plot();\n",
    "ax.set_ylabel(\"Percentage of delivery with no claim\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Link between SELLER_SCORE_COUNT and SELLER_SCORE_AVERAGE\n",
    "pd.crosstab(train_df.SELLER_SCORE_COUNT, train_df.SELLER_SCORE_AVERAGE[train_df.SELLER_SCORE_AVERAGE>40] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* Logically we can observe that the amount of product sold by the seller is growing while the number of claims is decreasing. This is true for all the claim types except one : SELLER_CANCEL_POSTERIORI percentage tends to be higher for the range 100000<1000000 of SELLER_SCORE_COUNT.\n",
    "\n",
    "* The null values of SELLER_SCORE_COUNT will be put at 0 in the mapping as it can correspond to seller without any product sold.\n",
    "\n",
    "* Just as for the last feature it is clear that seller with the highest score are the more reliable and result in a fewer number of claims. Especially seller with a mark of 49 close to the maximum have a number of claims significatively low. Thus it could be useful to add a feature to discriminate these top seller.\n",
    "\n",
    "* However an important point is that the best score possible 50 is reached only by a few individuals (51, while 3994 for 49 and 18006 for 48) and have in average in very bad reliability. We can also notice that these profiles all have sold less than 100 items. Thus, we can assume that some of these profiles are fake and manage somehow to get the maximum score to trick the system and get people trust. It could also be that those profiles have only a few sells hence the maximum score wich is impossible for a great number of sells.\n",
    "Anyway, as a result these profiles will be put in the same level as seller with bad scores.\n",
    "\n",
    "* We can add that without surprise their is an important correlation between SELLER_SCORE_AVERAGE and SELLER_SCORE_COUNT as reliable seller have in majority sold a lot of items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELLER_COUNTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "claim_percentage_crosstab('SELLER_COUNTRY')[:30] # Taking off country with a few number of deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Most of European countries have correct percentage of delivery.\n",
    "For instance: SWITZERLAND (50.1% no claim), GERMANY (60,9% no claim), BELGIUM (58,2% no claim), LUXEMBOURG (58,5% no claim).\n",
    "\n",
    "* However some European country perform bad like UNITED KINGDOM\tthat have notably high percentage of NOT_RECEIVED claims (19.2%).\n",
    "\n",
    "* It is hard to classify these country in classes as many countries have specificities (very high percentage of WITHDRAWAL claims for ITALY, overall high percentage of no claim for UNITED STATES however very high percentage of NOT_RECEIVED claims etc...)\n",
    "We then need to add as many dummy variables as there are countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELLER_DEPARTMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating SELLER_REGION : (we use the same department mapping as for BUYER_REGION)\n",
    "for dataset in combine:\n",
    "    dataset['SELLER_REGION'] = dataset['SELLER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    del dataset['SELLER_DEPARTMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each claim given SELLER_REGION\n",
    "claim_percentage_crosstab('SELLER_REGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* Unlike with BUYER_REGION where no correlation was noticeable with CLAIM_TYPE, here there are some differences between the places where the seller sends the command: commands sent from Paris (59,9% no claim) is overall more reliable than commands from Pays_De_La_Loire (34.2% no claim) for example.\n",
    "Thus we need to turn BUYER_REGION into dummy variables.\n",
    "\n",
    "* What's more, it could be unteresting to create an additionnal feature corresponding to commands sent and received in the same region (where SELLER_REGION=BUYER_REGION), since these commands may be more reliable in average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRODUCT_TYPE and PRODUCT_FAMILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given PRODUCT_TYPE\n",
    "claim_percentage_crosstab('PRODUCT_TYPE')[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given PRODUCT_FAMILY\n",
    "claim_percentage_crosstab('PRODUCT_FAMILY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations :\n",
    "\n",
    "* As PRODUCT_FAMILY already condensate most of information about products, we keep only a few label from PRODUCT_TYPE which stand out from the other labels are are not already in PRODUCT_FAMILY (Books in both for example). \n",
    "Among variables we can keep among PRODUCT_TYPE: PLAY CARDS (83.9% no claim), CD (60,5% no claim), COSMETIC (37.4% no claim), CELLPHONE (31,2% no claim).\n",
    "\n",
    "* ELECTRONICS are more likely to lead to DAMAGED claims.\n",
    "Indeed, electronic devices such as Television are more fragile and can be damaged during transport.\n",
    "On the contrary Wine products are unlikely to be damaged, certainly due to the special care and protections set for the transport since Wine are luxury products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ITEM_PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "describe(train_df['ITEM_PRICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of each claim given ITEM_PRICE\n",
    "claim_percentage_crosstab('ITEM_PRICE').reindex(['<10','10<20','20<50','50<100','100<500','500<1000','1000<5000','>5000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Cheap products are less likely to lead to claims.\n",
    "\n",
    "* Pricey products are more likely to lead to WITHDRAWAL/UNDEFINED claims.\n",
    "\n",
    "* Cheap products are more likely to lead to FAKE/NOT_RECEIVED products.\n",
    "\n",
    "* Products in range 100<500 typically lead more to DAMAGED claims in average.\n",
    "This corresponds to the price range of electronics, which are more likely to lead to DAMAGED mentions as we just saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will be used to create new variables and remove some others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maps a categorical variable 'col' given a dictionnary 'mappping'.\n",
    "# For example, calling this function with col = 'SHIPPING_PRICE' and \n",
    "# the mapping {\"<1\": 1, \"1<5\": 2, \"5<10\": 3, \"10<20\": 4, \">20\": 5, 0.0:0} \n",
    "# will change all rows having value \"<1\" to 1, \"1<5\" to 2, etc...\n",
    "def categorical_mapping(dataset, col, mapping):\n",
    "    dataset[col] = dataset[col].fillna(0).map(mapping).astype(int)\n",
    "def categorical_mapping_continuous(dataset, col, mapping):\n",
    "    dataset[col] = dataset[col].fillna(0).map(mapping).astype(float)\n",
    "\n",
    "# Turns a categorical variable into dummy variables.\n",
    "# For example calling this with col = 'SHIPPING_MODE'\n",
    "# will replace the 'SHIPPING_MODE' column with new columns \n",
    "# named 'SHIPPING_MODE_MONDIAL_RELAY_PREPAYE', 'SHIPPING_MODE_NORMAL'\n",
    "# 'SHIPPING_MODE_PICKUP', 'SHIPPING_MODE_RECOMMANDE', etc...\n",
    "# which values are all zero save for one.\n",
    "def categorical_to_dummy(dataset, col, dummy_na=False):\n",
    "    dummy = pd.get_dummies(dataset[col], prefix=col, dummy_na=dummy_na)\n",
    "    for column in dummy.columns.values:\n",
    "        dataset[column] = dummy[column]\n",
    "    del dataset[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some catgeorical variables have levels that are not represented in both the \n",
    "# training and testing set (or not represented in both the training/validation set).\n",
    "# We need to remove these levels if we want to turn the variables into dummies\n",
    "def SELLER_COUNTRY_map(c):\n",
    "    delete = ['IRELAND','LIECHTENSTEIN','SLOVAKIA (Slovak Republic)','ISRAEL','GIBRALTAR',\n",
    "              'PORTUGAL','SLOVENIA','LATVIA','MALTA','MALAYSIA','SWEDEN',\n",
    "              'VATICAN CITY STATE (HOLY SEE)','MARTINIQUE','LITHUANIA','JERSEY','ROMANIA',\n",
    "              'GUYANA','GREECE','ESTONIA','CYPRUS',]\n",
    "    if c in delete:\n",
    "        return 'OTHER'\n",
    "    return c\n",
    "\n",
    "def PRODUCT_TYPE_map(c):\n",
    "    delete = ['ACTIVITE_LOISIRS','AMPLIFICATOR',\n",
    "              'AUTORADIOS','CYCLE', 'INSOLITE', 'RADIO RECEPTOR', 'TELESCOPE',\n",
    "              'ANSWERING MACHINE', 'AUTOGRAPHES', 'INPUT ADAPTERS',\n",
    "              'MUSIC LOT']\n",
    "    if c in delete:\n",
    "        return 'OTHER'\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "claim_list = ['-', 'NOT_RECEIVED', 'SELLER_CANCEL_POSTERIORI', 'WITHDRAWAL', 'DAMAGED', 'DIFFERENT', \n",
    "                  'UNDEFINED', 'FAKE']\n",
    "\n",
    "# Helper function that transform both the training dataset \n",
    "# and the dataset for which we want to make prediction into\n",
    "# datasets that can be used by our machine learning algorithms\n",
    "# 'dataset_transform_func' is called on each dataset to perform \n",
    "# the feature engineering\n",
    "# Returns:\n",
    "# - train_X : the training dataset features\n",
    "# - train_Y : the training dataset outputs (in categorical form)\n",
    "# - train_Y_multiclass : the training dataset outputs (in dummy-encoding form)\n",
    "# - train_Y_mapped : the training dataset outputs (in one single vector of integers)\n",
    "# - testing_X : the features of the dataset for which we want to make predictions\n",
    "def create_model(dataset_transform_func):\n",
    "    training_df = train_df.copy()\n",
    "    testing_df = test_X.copy()\n",
    "    \n",
    "    dataset_transform_func(training_df)\n",
    "    dataset_transform_func(testing_df)\n",
    "    \n",
    "    #train_X = training_df.drop('CLAIM_TYPE',1).astype(int)\n",
    "    train_X = training_df.drop('CLAIM_TYPE',1)\n",
    "    train_Y = training_df.CLAIM_TYPE\n",
    "    testing_X = testing_df.drop('ID',1)\n",
    "\n",
    "    train_Y_multiclass = label_binarize(train_Y, classes=[\"-\",\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    \n",
    "\n",
    "    # Label encoding :\n",
    "    claim_mapping = {}\n",
    "    for i in range(8):\n",
    "        claim_mapping[claim_list[i]]=i\n",
    "\n",
    "    # Mapping\n",
    "    train_Y_mapped = pd.DataFrame(train_Y)\n",
    "    train_Y_mapped = train_Y_mapped.CLAIM_TYPE.map(claim_mapping)\n",
    "    train_Y_mapped = np.array(list(train_Y_mapped))\n",
    "    \n",
    "    return train_X, train_Y, train_Y_multiclass, testing_X, train_Y_mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score is used for this challenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computes the ROC AUC score.\n",
    "# Dummy enconding is used to represent both the predictions \n",
    "# and true values, i.e. the input are matrices and each \n",
    "# row is a vector containing a single 1 and zeros elsewhere.\n",
    "def auc_weighted(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred, average='weighted')\n",
    "    return score\n",
    "\n",
    "# Version used for training y as a vector of integers\n",
    "def auc_weighted_bis(y_true, y_pred):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_ =  lb.fit_transform(y_true)\n",
    "    y_pred_ = lb.transform(y_pred)\n",
    "    score = roc_auc_score(y_true_, y_pred_, average='weighted')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the challenge is to make the best predictions on a data set for which we don't have the `CLAIM_TYPE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_claims_prediction(preds):\n",
    "    classes = [\"-\",\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\"SELLER_CANCEL_POSTERIORI\",\n",
    "             \"NOT_RECEIVED\", \"UNDEFINED\",\"FAKE\"]\n",
    "    def claim_type(p):\n",
    "        return classes[np.argmax(p)]\n",
    "    df = pd.DataFrame(index=np.arange(0, len(preds)), columns=('ID', 'CLAIM_TYPE') )\n",
    "    df['ID'] = [i+100000 for i in range(len(preds))]\n",
    "    df['CLAIM_TYPE'] = [claim_type(p) for p in preds]\n",
    "    df.to_csv('prediction.csv', sep=';', index=False)\n",
    "\n",
    "def predict_claims(predictor, dataset):\n",
    "    preds = predictor.predict(dataset)\n",
    "    save_claims_prediction(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1 (by Fabien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our fist attempt. We start by preprocessing the dataset (creating new features, removing columns), then we use a RandomForest classifier (provided by scikit-learn) to predict the claims for the testing set (for which we really don't know the output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do :\n",
    "- Categorical variables need to be transformed to numeric variables\n",
    "- Fill missing values in variables\n",
    "- Creation of new variables /  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model1_feature_engineering(df):\n",
    "    # Turning SHIPPING_MODE into dummy variables:\n",
    "    categorical_to_dummy(df, 'SHIPPING_MODE')\n",
    "    \n",
    "    # Mapping SHIPPING_PRICE:\n",
    "    # Considering that Null values correspond to the case when shipping is free\n",
    "    SHIPPING_PRICE_mapping = {\"<1\": 1, \"1<5\": 2, \"5<10\": 3, \"10<20\": 4, \">20\": 5, 0.0:0} \n",
    "    categorical_mapping(df, 'SHIPPING_PRICE', SHIPPING_PRICE_mapping)\n",
    "    \n",
    "    # Mapping WARRANTIES_PRICE :\n",
    "    WARRANTIES_PRICE_mapping = {\"<5\": 1, \"5<20\": 2, \"20<50\": 3, \"50<100\": 4, \"100<500\": 5, 0.0:0}\n",
    "    categorical_mapping(df, 'WARRANTIES_PRICE', WARRANTIES_PRICE_mapping)\n",
    "    \n",
    "    # Mapping WARRANTIES_FLG :\n",
    "    WARRANTIES_FLG_mapping = {True: 1, False: 0}\n",
    "    categorical_mapping(df, 'WARRANTIES_FLG', WARRANTIES_FLG_mapping)\n",
    "    \n",
    "    # Mapping PRICECLUB_STATUS :\n",
    "    PRICECLUB_STATUS_mapping = {\"UNSUBSCRIBED\": 0, \"REGULAR\": 1, \"PLATINUM\": 2, \"SILVER\": 3, \"GOLD\": 4, 0.0:0}\n",
    "    categorical_mapping(df, 'PRICECLUB_STATUS', PRICECLUB_STATUS_mapping)\n",
    "    \n",
    "    # Mapping PURCHASE_COUNT :\n",
    "    PURCHASE_COUNT_mapping = {'<5':0,'5<20':1,'20<50':2,'50<100':3,'100<500':4,'>500':5}\n",
    "    categorical_mapping(df, 'PURCHASE_COUNT', PURCHASE_COUNT_mapping)\n",
    "    \n",
    "    # Creation of UNEXPERIENCED_BUYER :\n",
    "    df['UNEXPERIENCED_BUYER'] = np.where(df['PURCHASE_COUNT']==0, 1, 0)\n",
    "\n",
    "    # Simplification of feature REGISTRATION_DATE, \n",
    "    # replaced by BUYER_SENIORITY which corresponds to the number of years the buyer is using the service\n",
    "    df['BUYER_SENIORITY'] = 2017 - df['REGISTRATION_DATE']\n",
    "    del df['REGISTRATION_DATE']\n",
    "    \n",
    "    # Simplification of feature BUYER_BIRTHDAY_DATE, replaced by BUYER_AGE\n",
    "    df['BUYER_AGE'] = 2017 - df['BUYER_BIRTHDAY_DATE']\n",
    "    del df['BUYER_BIRTHDAY_DATE']\n",
    "        \n",
    "    # Filling missing data with ffill method:\n",
    "    df['BUYER_AGE'] = df['BUYER_AGE'].fillna(method='ffill').astype(int)\n",
    "        \n",
    "    # TODO: filling missing data with KNN\n",
    "        \n",
    "    # Creation of BUYER_IS_ABROAD\n",
    "    df['BUYER_IS_ABROAD'] = np.where(df['BUYER_REGION']=='Etranger', 1, 0)\n",
    "\n",
    "    # Build a mapping dictionnary from departments to regions\n",
    "    DEPARTMENT_mapping = {}\n",
    "\n",
    "    Bretagne = [22,29,35,56]\n",
    "    Normandie = [14, 27, 50, 61, 76]\n",
    "    Hauts_De_France = [2,59,60,62,80]\n",
    "    Ile_De_France = [77,78,91,95]\n",
    "    Paris = [75,92,93,94]\n",
    "    Grand_Est =[8,10,51,52,54,55,57,67,68,88]\n",
    "    Bourgogne_France_Compte = [21,25,39,58,70,71,89,90]\n",
    "    Nouvelle_Aquitaine = [16,17,19,23,24,33,40,47,64,79,86,87]\n",
    "    Auvergne_Rhone_Alpes = [1,3,7,15,26,38,42,43,63,69,73,74]\n",
    "    Occitanie = [9,11,12,30,31,32,34,46,48,65,66,81,82]\n",
    "    Centre_Val_De_Loire = [18,28,36,37,41,45]\n",
    "    PACA = [4,5,6,13,83,84]\n",
    "    Pays_De_La_Loire = [44,49,53,72,85]\n",
    "    Etranger = [-1,20,97] # Putting out of France departments into Foreign list, 20=Corse, 97=DOM/TOM\n",
    "    Null = [0,96,98,99,100] # null values (department 97 and 98 do not exist)\n",
    "\n",
    "    Regions = [Bretagne,Normandie,Hauts_De_France,Ile_De_France,Paris,Grand_Est,Bourgogne_France_Compte,Nouvelle_Aquitaine,\n",
    "               Auvergne_Rhone_Alpes,Occitanie,Centre_Val_De_Loire,PACA,Pays_De_La_Loire,Etranger,Null]\n",
    "\n",
    "    Region_names = ['Bretagne','Normandie','Hauts_De_France','Ile_De_France','Paris','Grand_Est','Bourgogne_France_Compte',\n",
    "                    'Nouvelle_Aquitaine','Auvergne_Rhone_Alpes','Occitanie','Centre_Val_De_Loire','PACA',\n",
    "                    'Pays_De_La_Loire','Etranger','Null']\n",
    "\n",
    "    for ind, region in enumerate(Regions):\n",
    "        for i in region:\n",
    "            DEPARTMENT_mapping[i]=Region_names[ind]\n",
    "           \n",
    "    # Creating BUYER_REGION \n",
    "    df['BUYER_REGION'] = df['BUYER_DEPARTMENT'].map(DEPARTMENT_mapping)    \n",
    "            \n",
    "    # Creation of BUYER_IS_ABROAD\n",
    "    df['BUYER_IS_ABROAD'] = np.where(df['BUYER_REGION']=='Etranger', 1, 0)\n",
    "\n",
    "    # Turning BUYER_REGION into dummy_variables\n",
    "    dummy = pd.get_dummies(df['BUYER_REGION'], prefix='BUYER_REGION')\n",
    "    for column in dummy.columns.values:\n",
    "        df[column] = dummy[column]\n",
    "\n",
    "    # Turning BUYER_DEPARTMENT into dummy variables:\n",
    "    dummy = pd.get_dummies(df['BUYER_DEPARTMENT'], prefix='BUYER_DEPARTMENT')\n",
    "    for column in dummy.columns.values:\n",
    "        df[column] = dummy[column] \n",
    "    \n",
    "    # Creating SELLER_REGION : (we use the same department mapping as for BUYER_REGION)\n",
    "    df['SELLER_REGION'] = df['SELLER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "            \n",
    "    # Creation of SELLER_IS_ABROAD\n",
    "    df['SELLER_IS_ABROAD'] = np.where(df['SELLER_REGION']=='Etranger', 1, 0)   \n",
    "\n",
    "    # Turning SELLER_REGION into dummy_variables\n",
    "    dummy = pd.get_dummies(df['SELLER_REGION'], prefix='SELLER_REGION')\n",
    "    for column in dummy.columns.values:\n",
    "        df[column] = dummy[column]\n",
    "\n",
    "    # Turning SELLER_DEPARTMENT into dummy variables:\n",
    "    dummy = pd.get_dummies(df['SELLER_DEPARTMENT'], prefix='SELLER_DEPARTMENT')\n",
    "    for column in dummy.columns.values:\n",
    "        df[column] = dummy[column]\n",
    "\n",
    "    # Creating SAME_REGION_BUYER_SELLER equal to 1 when Buyer and Seller are from the same region\n",
    "    df['SAME_REGION_BUYER_SELLER'] = np.where(dataset['SELLER_REGION']==dataset['BUYER_REGION'], 1, 0)\n",
    "\n",
    "    # Creating SAME_DEPARTMENT_BUYER_SELLER equal to 1 when Buyer and Seller are from the same department\n",
    "    df['SAME_DEPARTMENT_BUYER_SELLER'] = np.where(dataset['SELLER_DEPARTMENT']==dataset['BUYER_DEPARTMENT'], 1, 0)\n",
    "    \n",
    "    Region_names_dist = ['Bretagne','Normandie','Hauts_De_France','Ile_De_France','Grand_Est','Bourgogne_France_Compte',\n",
    "                        'Nouvelle_Aquitaine','Auvergne_Rhone_Alpes','Occitanie','Centre_Val_De_Loire','PACA',\n",
    "                        'Pays_De_La_Loire']\n",
    "    \n",
    "    # Distance in kms between seller region and buyer region:\n",
    "    distance_matrix = pd.DataFrame(np.zeros((12,12)), columns=Region_names_dist, index=Region_names_dist)\n",
    "    distance_matrix['Bretagne']=[0,311,564,349,830,617,466,738,705,302,1046,113]\n",
    "    distance_matrix['Normandie']=[0,0,257,136,639,444,655,595,787,241,904,387]\n",
    "    distance_matrix['Hauts_De_France']=[0,0,0,225,525,502,800,692,895,348,1001,600]\n",
    "    distance_matrix['Ile_De_France']=[0,0,0,0,492,315,584,466,679,133,775,385]\n",
    "    distance_matrix['Grand_Est']=[0,0,0,0,0,330,969,493,971,587,802,865]\n",
    "    distance_matrix['Bourgogne_France_Compte']=[0,0,0,0,0,0,671,195,673,315,504,639]\n",
    "    distance_matrix['Nouvelle_Aquitaine']=[0,0,0,0,0,0,0,556,246,468,646,353]\n",
    "    distance_matrix['Auvergne_Rhone_Alpes']=[0,0,0,0,0,0,0,0,537,466,314,685]\n",
    "    distance_matrix['Occitanie']=[0,0,0,0,0,0,0,0,0,555,404,585]\n",
    "    distance_matrix['Centre_Val_De_Loire']=[0,0,0,0,0,0,0,0,0,0,758,335]\n",
    "    distance_matrix['PACA']=[0,0,0,0,0,0,0,0,0,0,0,986]\n",
    "    distance_matrix['Pays_De_La_Loire']=[0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    mat = distance_matrix.values\n",
    "    mat = mat + np.transpose(mat)\n",
    "    distance_matrix = pd.DataFrame(mat, columns=Region_names_dist, index=Region_names_dist)\n",
    "\n",
    "    # Mapping distance matrix:\n",
    "    distance_matrix = distance_matrix.apply(lambda x: round(x/100,0)).astype(int)\n",
    "\n",
    "    # Creation of SELLER_BUYER_REGION_DISTANCE\n",
    "    distances = []\n",
    "    for i in range(len(df)):\n",
    "        buyer_region = df['BUYER_REGION'][i]\n",
    "        seller_region = df['SELLER_REGION'][i]\n",
    "        if buyer_region==\"Paris\":\n",
    "            buyer_region=\"Ile_De_France\"\n",
    "        if seller_region==\"Paris\":\n",
    "            seller_region=\"Ile_De_France\"\n",
    "        if buyer_region==\"Etranger\" or seller_region==\"Etranger\":\n",
    "            distances.append(11)\n",
    "        elif buyer_region==\"Null\" or seller_region==\"Null\":\n",
    "            distances.append(5)\n",
    "        else:\n",
    "            distances.append(distance_matrix.loc[buyer_region, seller_region]) \n",
    "    df['SELLER_BUYER_REGION_DISTANCE'] = distances\n",
    "    \n",
    "    # Now we can delete BUYER_REGION, BUYER_DEPARTMENT, SELLER_REGION and SELLER_DEPARTMENT\n",
    "    del df['BUYER_REGION']\n",
    "    del df['BUYER_DEPARTMENT']\n",
    "    del df['SELLER_REGION']\n",
    "    del df['SELLER_DEPARTMENT']\n",
    "    \n",
    "    \n",
    "    # Mapping BUYING_DATE into numerical variable: '3/2017' -> 3\n",
    "    # Using regular expression to isolate months\n",
    "    df['BUYING_DATE'] = df['BUYING_DATE'].apply( lambda s : int(re.findall('[0-9]*',s)[0]) )\n",
    "    \n",
    "    categorical_to_dummy(df, 'BUYING_DATE')\n",
    "    \n",
    "    # Mapping SELLER_SCORE_COUNT\n",
    "    SELLER_SCORE_COUNT_mapping = {0.0:0, '<100':1, '100<1000':2, '1000<10000':3, '10000<100000':4, '100000<1000000':5}\n",
    "    categorical_mapping(df, 'SELLER_SCORE_COUNT', SELLER_SCORE_COUNT_mapping)\n",
    "    \n",
    "    # Mapping SELLER_SCORE_AVERAGE\n",
    "    def SELLER_SCORE_AVERAGE_map(score):\n",
    "        if score == 50:\n",
    "            return -1\n",
    "        elif score < 44:\n",
    "            return 0\n",
    "        elif score < 46:\n",
    "            return 1\n",
    "        elif score == 46:\n",
    "            return 2\n",
    "        elif score == 47:\n",
    "            return 3\n",
    "        elif score == 48:\n",
    "            return 4\n",
    "        elif score == 49:\n",
    "            return 5\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df['SELLER_SCORE_AVERAGE'] = df['SELLER_SCORE_AVERAGE'].apply(SELLER_SCORE_AVERAGE_map)\n",
    "    \n",
    "    # Create VIP_SELLER feature for seller with a score of 49 (label 5)\n",
    "    df['VIP_SELLER'] = np.where(df['SELLER_SCORE_AVERAGE']==5, 1, 0)\n",
    "    \n",
    "    # Turning SELLER_COUNTRY into dummy variables:\n",
    "    dummy = pd.get_dummies(df['SELLER_COUNTRY'], prefix='SELLER_COUNTRY')\n",
    "    for column in dummy.columns.values:\n",
    "        df[column] = dummy[column]\n",
    "    \n",
    "    # Creation of a feature representing seller country distance to France by mapping SELLER_COUNTRY:\n",
    "    countries = set()\n",
    "    for country in df['SELLER_COUNTRY']:\n",
    "        countries.add(country)\n",
    "    countries = list(countries)\n",
    "    \n",
    "    distances=[1,1,2,2,1,2,2,2,1,2,3,4,0,1,1,1,1,1,2,1,1,4,1,2,4,1,4,2,4,2,4,1,2,2,0,4,3,4,4]\n",
    "\n",
    "    SELLER_COUNTRY_mapping = {}\n",
    "    for i in range(len(distances)):\n",
    "        SELLER_COUNTRY_mapping[countries[i]] = distances[i]\n",
    "    SELLER_COUNTRY_mapping\n",
    "    \n",
    "    categorical_mapping(df, 'SELLER_COUNTRY', SELLER_COUNTRY_mapping)\n",
    "    \n",
    "    # Turning PRODUCT_FAMILY into dummy_variables\n",
    "    categorical_to_dummy(df, 'PRODUCT_FAMILY')\n",
    "\n",
    "    # Turning PRODUCT_FAMILY into dummy_variables\n",
    "    categorical_to_dummy('PRODUCT_TYPE')\n",
    "    \n",
    "    # Mapping ITEM_PRICE:\n",
    "    ITEM_PRICE_mapping = {\"<10\" : 1, \"10<20\" : 2, \"20<50\" : 3, \"50<100\" : 4, \"100<500\" : 5, \"500<1000\" : 6,\n",
    "                          \"1000<5000\" : 7, \">5000\" : 8} \n",
    "    categorical_mapping(df, 'ITEM_PRICE', ITEM_PRICE_mapping)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create all datasets that are necessary to train, validate and test models\n",
    "train_X_full, train_Y_full, train_Y_multiclass_full, testing_X = create_model(model1_feature_engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary of features\n",
    "print(testing_X.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X_full, train_Y_multiclass_full, \n",
    "                                                      train_size=.7, random_state=7)\n",
    "\n",
    "print (train_X_full.shape, train_Y_full.shape, train_X.shape, valid_X.shape, \n",
    "       train_Y.shape , valid_Y.shape , testing_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MACHINE LEARNING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_Y = [\"-\",\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\"UNDEFINED\",\"FAKE\"]\n",
    "index_Y_claims = [\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\"UNDEFINED\",\"FAKE\"]\n",
    "index_Y_bin = [\"no claim\", \"claim\"]\n",
    "\n",
    "# Fit the model and make predictions\n",
    "def pred(model):\n",
    "    begin = time.time()\n",
    "    model.fit(train_X, train_Y)\n",
    "    pred_Y = model.predict(valid_X)\n",
    "    print(round(time.time() - begin, 2), \" s\")\n",
    "    return pred_Y\n",
    "\n",
    "# Display the confusion matrix\n",
    "def display_cm(model, train_X, train_Y, valid_X, valid_Y):\n",
    "    model.fit(train_X, train_Y)\n",
    "    pred_Y = model.predict(valid_X)\n",
    "    cm = confusion_matrix(valid_Y, pred_Y)\n",
    "    cm_df = pd.DataFrame(cm, index=index_Y, columns=index_Y)\n",
    "    return cm_df\n",
    "\n",
    "# Return average AUC with KFold cross-validation \n",
    "def run_kfold(model):\n",
    "    begin = time.time()\n",
    "    kf = KFold(n_splits=10, shuffle=False, random_state=7)\n",
    "    outcomes = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(train_X_full):\n",
    "        fold += 1\n",
    "        X_train, X_valid = train_X_full.values[train_index], train_X_full.values[test_index]\n",
    "        #y_train, y_valid = train_Y_multiclass[train_index], train_Y_multiclass[test_index] # With dummy variables\n",
    "        y_train, y_valid = train_Y_mapped[train_index], train_Y_mapped[test_index] # Without dummy variables\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_valid)\n",
    "        \n",
    "        # Without dummy variables\n",
    "        lb = LabelBinarizer()\n",
    "        y_valid =  lb.fit_transform(y_valid)\n",
    "        y_pred = lb.transform(y_pred)\n",
    "        \n",
    "        \n",
    "        auc = auc_weighted(y_valid, y_pred)\n",
    "        outcomes.append(auc)\n",
    "        print(\"Fold {0}, AUC: {1}\".format(fold, round(auc,3)))     \n",
    "    mean_outcome = round(np.mean(outcomes),3)\n",
    "    print(\"Mean AUC: {0}\".format(mean_outcome)) \n",
    "    print(\"\\n\",round(time.time()-begin,2),\" s\")\n",
    "    return mean_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First predict claim/no claim, then predict the type of claim if there is claim\n",
    "\n",
    "def auc_2_steps(model_1, model_2):    \n",
    "    # Train test split with all data\n",
    "    global train_X, valid_X, train_Y, valid_Y, pred_Y\n",
    "    \n",
    "    # With dummy variables :\n",
    "    \n",
    "    #train_X, valid_X, train_Y_multi, valid_Y = train_test_split(train_X_full, train_Y_full, \n",
    "     #                                                           train_size=.7, random_state=7)\n",
    "    #train_Y = np.where(train_Y_multi!='-', 1, 0)\n",
    "    #valid_Y_bin = np.where(valid_Y!='-', 1, 0)\n",
    "        \n",
    "    # Without dummy variables :\n",
    "    \n",
    "    train_X, valid_X, train_Y_multi, valid_Y = train_test_split(train_X_full, train_Y_mapped, \n",
    "                                                                train_size=.7, random_state=7)\n",
    "    train_Y = np.where(train_Y_multi!=0, 1, 0)\n",
    "    valid_Y_bin = np.where(valid_Y!=0, 1, 0)   \n",
    "\n",
    "    # First prediction : claim or no claim\n",
    "    pred_Y_bin = pred(model_1)\n",
    "    auc = auc_weighted(valid_Y_bin, pred_Y_bin)\n",
    "    print(\"auc claim/no claim : \", auc)\n",
    "\n",
    "    # With dummy variables :\n",
    "    \n",
    "    #train_X = train_X[train_Y_multi != '-']\n",
    "    #train_Y = train_Y_multi[train_Y_multi != '-']\n",
    "    #lb2 = LabelBinarizer()\n",
    "    #train_Y = lb2.fit_transform(train_Y)\n",
    "    \n",
    "    # Without dummy variables :\n",
    "    \n",
    "    train_X = train_X[train_Y_multi != 0]\n",
    "    train_Y = train_Y_multi[train_Y_multi != 0]\n",
    "    \n",
    "    # Second prediction : if claim, predict type of claim\n",
    "    pred_Y_claims = pred(model_2)\n",
    "    #pred_Y_claims = lb2.inverse_transform(pred_Y_claims) # With dummy variables\n",
    "\n",
    "    pred_Y = []\n",
    "    for i in range(30000):\n",
    "        if pred_Y_bin[i]==0:\n",
    "            #pred_Y.append('-') # With dummy variables\n",
    "            pred_Y.append(0) # Without dummy variables\n",
    "        else:\n",
    "            pred_Y.append(pred_Y_claims[i])\n",
    "    pred_Y = np.array(pred_Y)\n",
    "\n",
    "    lb3 = LabelBinarizer()\n",
    "    valid_Y = lb3.fit_transform(valid_Y)\n",
    "    pred_Y = lb3.transform(pred_Y)\n",
    "\n",
    "    return auc_weighted(valid_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RandomForestClassifier(n_estimators=100)\n",
    "model_2 = OneVsRestClassifier(RandomForestClassifier(n_estimators=100))\n",
    "auc_rf_rf_onevsall = auc_2_steps(model_1, model_2)\n",
    "auc_rf_rf_onevsall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RandomForestClassifier(n_estimators=100)\n",
    "model_2 = RandomForestClassifier(n_estimators=100)\n",
    "auc_rf_rf = auc_2_steps(model_1, model_2)\n",
    "auc_rf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RandomForestClassifier(n_estimators=100)\n",
    "model_2 = KNeighborsClassifier(n_neighbors = 5)\n",
    "auc_rf_knn = auc_2_steps(model_1, model_2)\n",
    "auc_rf_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = KNeighborsClassifier(n_neighbors = 5)\n",
    "model_2 = RandomForestClassifier(n_estimators=100)\n",
    "auc_knn_rf = auc_2_steps(model_1, model_2)\n",
    "auc_knn_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RandomForestClassifier(n_estimators=100)\n",
    "model_2 = OneVsRestClassifier(xgb.XGBClassifier(n_estimators=200))\n",
    "auc_rf_gbm = auc_2_steps(model_1, model_2)\n",
    "auc_rf_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the models auc\n",
    "\n",
    "models = pd.DataFrame({\n",
    "    'Model 1': ['Random Forest', 'Random Forest', 'Random Forest', 'Random Forest', 'KNN'],\n",
    "    'Model 2': ['Random Forest', 'Random Forest OneVsAll', 'Gradient Boosting', 'KNN', 'Random Forest' ],\n",
    "    'AUC': [auc_rf_rf, auc_rf_rf_onevsall, auc_rf_gbm, auc_rf_knn, auc_knn_rf]\n",
    "})\n",
    "\n",
    "models.sort_values(by='AUC', ascending=False).style.set_table_styles([\n",
    "    {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions with all labels in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting with XGboost\n",
    "\n",
    "gbm = OneVsRestClassifier(xgb.XGBClassifier(n_estimators=50))\n",
    "pred_Y = pred(gbm)\n",
    "auc_gbm = round(auc_weighted(valid_Y, pred_Y),3)\n",
    "auc_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "auc_rf = run_kfold(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest OneVsRest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "auc_rf_onevsall = run_kfold(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "mlp = MLPClassifier()\n",
    "pred_Y = pred(mlp)\n",
    "auc_mlp = round(auc_weighted(valid_Y, pred_Y),3)\n",
    "auc_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "auc_decision_tree = run_kfold(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Tree \n",
    "\n",
    "extra_tree = ExtraTreeClassifier()\n",
    "auc_extra_tree = run_kfold(extra_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "auc_knn = run_kfold(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the models auc\n",
    "\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Gradient Boosting', 'Random Forest', 'Random Forest OneVsAll', \n",
    "              'MLP', 'Decision Tree', 'Extra Tree', 'KNN'],\n",
    "    'AUC': [auc_gbm, auc_rf, auc_rf_onevsall, auc_mlp, auc_decision_tree,\n",
    "            auc_extra_tree, auc_knn]\n",
    "})\n",
    "\n",
    "models.sort_values(by='AUC', ascending=False).style.set_table_styles([\n",
    "    {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = train_Y.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(valid_Y[:, i], pred_Y[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(valid_Y.ravel(), pred_Y.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "lw=2\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 (by Vincent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In his very first model (which is scarcely visible now), Fabien tried to predict directly the `CLAIM_TYPE` field using a Random Forest classifier in some new features such as `VIP_SELLER`, `BUYER_REGION`, `SAME_REGION_BUYER_SELLER`, etc...\n",
    "\n",
    "As I didn't know where to start, I decided to use the same feature set, but with a $k$-neighbors classifier. The results were worse. One problem of this approach is that the `-` level in the `CLAIM_TYPE` field is by far the most present (around 50% of orders did not lead to a claim); therefore when using 10 or 5 neighbors, `-` wins quite often (even when using `weights='distance'`). Knowing that, I tried to remove randomly from the training set a proportion of `-` : this led to better results both in the validation set and on the submitted test set; and confirmed that the problem was indeed due to the  fact that the classes are not well-balanced. However, removing randomly elements from the training set is not satisfactory and requires some knowledge on the testing set (which are not supposed to have).\n",
    "\n",
    "In my second attempt, I splitted the problem into two subproblems that ought to be simpler:\n",
    "- claim vs non-claim prediction;\n",
    "- claim kind prediction.\n",
    "I used a Random Forest to predict the presence of a claim and a KNeighborsClassifier to predict the claim type. \n",
    "I used the same features as before.\n",
    "This led to a significant improvement on both the validation set and testing set.\n",
    "\n",
    "I tried to improve the results with the same configuration using a custom metric defined as a Python function for the KNeighborsClassifier (the `metric` parameter of the class). This was too slow to be used.\n",
    "\n",
    "I then decided to change the features used with the KNeighborsClassifier. Indeed some features were encoded in a single variable using integers which implied a deceiving proximity between some levels. For example, the `WARRANTLIES_PRICE` were encoded as follow:\n",
    "- `Na` : `0` (absence of warrantly)\n",
    "- `'< 5'` : `1`  \n",
    "- `'5 < 20'` : `2`  \n",
    "- `'20 < 50'` : `3`  \n",
    "- `'50 < 100'` : `4`  \n",
    "\n",
    "Which implies that an order with warrantly price `'50 < 100'` is closer to one with `'20 < 50'` than to another with warantly price `'< 5'`. However, as far as the proportion of claim is concerned, this is not necessarily true. I switched most of the variables encoded this way into a list of rates indicating the proportion of each claim type.\n",
    "Also, some variables such as the `BUYER_AGE`, have a much bigger span the other variables. The $k$-neighbors were therefore often orders made by person with similar age, which is not bad per se, but might not be the most relevant variable to choose from. I did some kind of empirical weighting to improve the results. \n",
    "\n",
    "All these tweaks allowed me to reach rank #4 of the challenge at the time, outclassing Fabien by a significant marging (which is now reversed :'()\n",
    "The features used in the lastest version haven't evolved much since then.\n",
    "\n",
    "After a significant number of tries, I can say that the rate of correct prediction for claim vs. no-claim seems to top at 65%.\n",
    "For the sake of curiosity, I tried to use a neural network (Keras) with the features used by the Random Forest. The accuracy on the training set progresses slowly to a bit more than 70% with 40 epochs, and the accuracy on the validation seems to get stuck at 64% thus confirming that it will be hard to get more than 65% without adding useful external features.\n",
    "\n",
    "Since the majority of orders lead to no claim, predicting the presence of a claim correctly is important to get a good AUC score. Predicting the correct type of claim, if any, is of less significance.\n",
    "\n",
    "I slightly improved the results by reducing the number of neighbors used to predict the claim type to 1, and using a BaggingClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('train_X.csv', sep=',')\n",
    "train_Y = pd.read_csv('train_Y.csv', sep=';')\n",
    "test_X = pd.read_csv('test_X.csv', sep=',')\n",
    "train = pd.merge(train_X, train_Y, on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the presence of a claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "without_claims = []\n",
    "for i in range(len(train)):\n",
    "    if train.CLAIM_TYPE[i] == '-':\n",
    "        without_claims.append(i)\n",
    "train_claims = train.drop(without_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_claims), len(train)) # The two classes are well balanced ('good news everyone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = train\n",
    "train, valid = train_test_split(train.copy(), train_size=.7, random_state=42)\n",
    "#train, valid = train_test_split(train.copy(), train_size=.7, random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We start by defining a function that will transform the dataset (feature engineering)\n",
    "def data_transformation_claim_predictor(df):\n",
    "    if 'ID' in df.columns:\n",
    "        del df['ID']\n",
    "       \n",
    "    def CLAIM_TYPE_filter(c):\n",
    "        if c == '-':\n",
    "            return 0\n",
    "        return 1\n",
    "    if 'CLAIM_TYPE' in df.columns:\n",
    "        df['CLAIM'] = df['CLAIM_TYPE'].apply(CLAIM_TYPE_filter)\n",
    "        del df['CLAIM_TYPE']\n",
    "    \n",
    "    # Turning SHIPPING_MODE into dummy variables:\n",
    "    categorical_to_dummy(df, 'SHIPPING_MODE')\n",
    "    \n",
    "    # Mapping SHIPPING_PRICE:\n",
    "    # Considering that Null values correspond to the case when shipping is free\n",
    "    df['SHIPPING_PRICE'] = df['SHIPPING_PRICE'].fillna(value=0)\n",
    "    categorical_to_dummy(df, 'SHIPPING_PRICE')\n",
    "    \n",
    "    # Mapping WARRANTIES_PRICE :\n",
    "    df['WARRANTIES_PRICE'] = df['WARRANTIES_PRICE'].fillna(value='0')\n",
    "    categorical_to_dummy(df, 'WARRANTIES_PRICE')\n",
    "    \n",
    "    # Mapping WARRANTIES_FLG :\n",
    "    WARRANTIES_FLG_mapping = {True: 1, False: 0}\n",
    "    categorical_mapping(df, 'WARRANTIES_FLG', WARRANTIES_FLG_mapping)\n",
    "    \n",
    "    # Mapping PRICECLUB_STATUS :\n",
    "    df['PRICECLUB_STATUS'] = df['PRICECLUB_STATUS'].fillna(value='NA')\n",
    "    categorical_to_dummy(df, 'PRICECLUB_STATUS')\n",
    "    \n",
    "    # Mapping PURCHASE_COUNT :\n",
    "    categorical_to_dummy(df, 'PURCHASE_COUNT')\n",
    "        \n",
    "    df['BUYER_SENIORITY'] = (2017 - df['REGISTRATION_DATE'])\n",
    "    del df['REGISTRATION_DATE']\n",
    "    categorical_to_dummy(df, 'BUYER_SENIORITY')\n",
    "\n",
    "    # Filling missing data with ffill method:\n",
    "    df['BUYER_AGE_GROUP'] = group_by_age(20, 60, 5, df.BUYER_BIRTHDAY_DATE)\n",
    "    df['BUYER_AGE_GROUP'] = df['BUYER_AGE_GROUP'].fillna(method='ffill').astype(int)\n",
    "    categorical_to_dummy(df, 'BUYER_AGE_GROUP')\n",
    "    del df['BUYER_BIRTHDAY_DATE']\n",
    "    \n",
    "    # Mapping BUYER_REGION\n",
    "    df['BUYER_REGION'] = df['BUYER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    del df['BUYER_DEPARTMENT']\n",
    "    categorical_to_dummy(df, 'BUYER_REGION')\n",
    "    \n",
    "    df['BUYING_DATE'] = df['BUYING_DATE'].apply( lambda s : int(re.findall('[0-9]*',s)[0]) )\n",
    "    categorical_to_dummy(df, 'BUYING_DATE')\n",
    "\n",
    "    # Mapping SELLER_SCORE_COUNT\n",
    "    df['SELLER_SCORE_COUNT'] = df['SELLER_SCORE_COUNT'].fillna(value='NA')\n",
    "    categorical_to_dummy(df, 'SELLER_SCORE_COUNT')\n",
    "    \n",
    "    df['SELLER_SCORE_AVERAGE'] = df['SELLER_SCORE_AVERAGE'].fillna(value=0).astype(int)\n",
    "\n",
    "    # Processing SELLER_COUNTRY variable:\n",
    "    df['SELLER_COUNTRY'] = df['SELLER_COUNTRY'].fillna(value='OTHER').apply(SELLER_COUNTRY_map)\n",
    "    categorical_to_dummy(df, 'SELLER_COUNTRY')\n",
    "\n",
    "    df['SELLER_REGION'] = df['SELLER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    categorical_to_dummy(df, 'SELLER_REGION')\n",
    "    del df['SELLER_DEPARTMENT']\n",
    "\n",
    "    # Turning PRODUCT_FAMILY into dummy_variables\n",
    "    categorical_to_dummy(df, 'PRODUCT_FAMILY')\n",
    "\n",
    "    # PRODUCT_TYPE\n",
    "    df['PRODUCT_TYPE'] = df['PRODUCT_TYPE'].fillna(value='OTHER').apply(PRODUCT_TYPE_map)\n",
    "    categorical_to_dummy(df, 'PRODUCT_TYPE')\n",
    "    \n",
    "    \n",
    "    # Mapping ITEM_PRICE:\n",
    "    categorical_to_dummy(df, 'ITEM_PRICE')\n",
    "    if 'ITEM_PRICE_>5000' in df.columns:\n",
    "        del df['ITEM_PRICE_>5000']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Then we can fit either a RandomForest or a neural network\n",
    "def build_random_forest_regressor():\n",
    "    t = train.copy()\n",
    "    data_transformation_claim_predictor(t)\n",
    "    v = valid.copy()\n",
    "    data_transformation_claim_predictor(v)\n",
    "    \n",
    "    t_X = t.drop('CLAIM',1)\n",
    "    t_Y = t.CLAIM\n",
    "    v_X = v.drop('CLAIM',1)\n",
    "    v_Y = v.CLAIM\n",
    "    \n",
    "    # Since there are only two classes (claim vs non-claim), we can use a Regressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    #rf = RandomForestRegressor(verbose=1, n_jobs=2, n_estimators = 200, max_features=int(len(t_X.columns)/3), max_depth=30)\n",
    "    #rf = RandomForestRegressor(verbose=1, n_jobs=2, n_estimators = 100, max_features=int(len(t_X.columns)/3), max_depth=50)\n",
    "    rf = RandomForestRegressor(verbose=1, n_jobs=2, n_estimators = 20, max_features=int(len(t_X.columns)/3), min_samples_split=100)\n",
    "    rf.fit(t_X, t_Y)\n",
    "    s = rf.score(v_X, v_Y)\n",
    "    print(\"Score (MSE) : \", s)\n",
    "    pred_Y = rf.predict(v_X)\n",
    "    in_middle = ((pred_Y > 0.45) * (pred_Y < 0.55))\n",
    "    pred_Y = (pred_Y > 0.5) * 1\n",
    "    s = (v_Y == pred_Y).sum()\n",
    "    print(\"Score (accuracy) : \", (s/len(v_X)))\n",
    "    print(\"Proportion of claims:\", (pred_Y.sum() / len(pred_Y)))\n",
    "    print(\"Proportion of in-between:\", (in_middle.sum() / len(pred_Y)))\n",
    "    return rf\n",
    "\n",
    "def build_deep_claim_predictor():\n",
    "    t = train.copy()\n",
    "    data_transformation_claim_predictor(t)\n",
    "    v = valid.copy()\n",
    "    data_transformation_claim_predictor(v)\n",
    "    \n",
    "    t_X = t.drop('CLAIM',1)\n",
    "    t_Y = t.CLAIM\n",
    "    v_X = v.drop('CLAIM',1)\n",
    "    v_Y = v.CLAIM\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    t_Y_cat = to_categorical(t_Y)\n",
    "    v_Y_cat = to_categorical(v_Y)\n",
    "    \n",
    "    # Keras has some problems with Pandas DataFrame, so we extract the raw values\n",
    "    t_X = t_X.values\n",
    "    v_X = v_X.values\n",
    "\n",
    "    print(len(t_X[0]), \"input variables\")\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import Dense\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(input_shape=(len(t_X[0]),), units=128, activation='relu'))\n",
    "    classifier.add(Dropout(0.3)) \n",
    "    classifier.add(Dense(units = 64, activation='relu'))\n",
    "    classifier.add(Dropout(0.3)) \n",
    "    classifier.add(Dense(units = 32, activation='relu'))\n",
    "    classifier.add(Dropout(0.3)) # Overfitting reduction - Dropout\n",
    "    classifier.add(Dense(units = 2, activation='softmax'))\n",
    "    classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    classifier.fit(t_X, t_Y_cat, validation_data = (v_X, v_Y_cat), epochs = 64, batch_size=256)\n",
    "    score = classifier.evaluate(v_X, v_Y_cat, batch_size=32)    \n",
    "    print(\"Score (loss, acc) : \", score)\n",
    "    pred_Y = classifier.predict(v_X)\n",
    "    print(\"Proportion of claims:\", (pred_Y[:,1].sum() / len(pred_Y)))\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = build_random_forest_regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays features importance\n",
    "df = train.copy()\n",
    "data_transformation_claim_predictor(df)\n",
    "feature_names = df.drop('CLAIM',1).columns\n",
    "imp = rf.feature_importances_ \n",
    "print(len(imp), \"features\")\n",
    "for i in range(len(imp)):\n",
    "    print(feature_names[i], ':', imp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deep = build_deep_claim_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(deep).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both models seem to reach a maximum peak at around 64% of correctly predicted claims. \n",
    "It seems it is going to be hard doing better without adding more variables that introduce additional information.\n",
    "As the \"-\" class is the most present one, this score will considerably limit the score we can get on the challenge data website.\n",
    "\n",
    "Trying to add distances between seller and buyer as Fabien did seems relevant. However, I am not convinced that it will fundamentaly change the maximum score we get get. Indeed, predicting claims is hard as two extremely similar orders may lead to different claims (or no claims) depending on external factors that are not present in the dataset and difficult to evaluate.\n",
    "\n",
    "Moreover, all this work may be a waste of time ! By using only three variables (SELLER_DEPARTMENT, SELLER_SCORE_COUNT and SELLER_SCORE_AVERAGE), I correctly predicted around 62% of orders leading to a claim (by creating a map between these three variables and the percentage of claims corresponding to the tuple). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of claim type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transformation_claim_type_classifier(df):\n",
    "    if 'ID' in df.columns:\n",
    "        del df['ID']\n",
    "    \n",
    "    # Mapping SHIPPING_MODE\n",
    "    crosstab = pd.crosstab(train['SHIPPING_MODE'], train.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        SHIPPING_MODE_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['SHIPPING_MODE_NOTE_' + str(i)] = df['SHIPPING_MODE'].map(SHIPPING_MODE_mapping).fillna(0)\n",
    "        df['SHIPPING_MODE_NOTE_' + str(i)] = df['SHIPPING_MODE_NOTE_' + str(i)]*400\n",
    "    del df['SHIPPING_MODE']\n",
    "    \n",
    "    # Mapping SHIPPING_PRICE:\n",
    "    # Considering that Null values correspond to the case when shipping is free\n",
    "    SHIPPING_PRICE_mapping = {\"<1\": 1, \"1<5\": 2, \"5<10\": 3, \"10<20\": 4, \">20\": 5, 0.0:0} \n",
    "    categorical_mapping(df, 'SHIPPING_PRICE', SHIPPING_PRICE_mapping)\n",
    "    df['SHIPPING_PRICE'] = df['SHIPPING_PRICE'] * 4\n",
    "    \n",
    "    # Mapping WARRANTIES_PRICE :\n",
    "    WARRANTIES_PRICE_mapping = {\"<5\": 1, \"5<20\": 2, \"20<50\": 3, \"50<100\": 4, \"100<500\": 5, 0.0:0}\n",
    "    categorical_mapping(df, 'WARRANTIES_PRICE', WARRANTIES_PRICE_mapping)\n",
    "    df['WARRANTIES_PRICE'] = df['WARRANTIES_PRICE'] * 4\n",
    "    \n",
    "    # Removing WARRANTIES_FLG :\n",
    "    del df['WARRANTIES_FLG']\n",
    "    \n",
    "    df['CARD_PAYMENT'] = df['CARD_PAYMENT'] * 0.25\n",
    "    df['COUPON_PAYMENT'] = df['COUPON_PAYMENT'] * 0.25\n",
    "    df['RSP_PAYMENT'] = df['RSP_PAYMENT'] * 0.25\n",
    "    df['WALLET_PAYMENT'] = df['WALLET_PAYMENT'] * 0.25\n",
    "    \n",
    "    # Mapping PRICECLUB_STATUS :\n",
    "    PRICECLUB_STATUS_mapping = {\"UNSUBSCRIBED\": 0, \"REGULAR\": 0.25, \"PLATINUM\": 0.5, \"SILVER\": 0.75, \"GOLD\": 1, 0.0:0}\n",
    "    categorical_mapping_continuous(df, 'PRICECLUB_STATUS', PRICECLUB_STATUS_mapping)\n",
    "    df['PRICECLUB_STATUS'] = df['PRICECLUB_STATUS'] * 10\n",
    "\n",
    "    # Mapping PURCHASE_COUNT :\n",
    "    PURCHASE_COUNT_mapping = {'<5':0,'5<20':1,'20<50':2,'50<100':3,'100<500':4,'>500':5}\n",
    "    categorical_mapping(df, 'PURCHASE_COUNT', PURCHASE_COUNT_mapping)\n",
    "    df['PRICECLUB_STATUS'] = df['PRICECLUB_STATUS'] * 4\n",
    "\n",
    "    # Simplification of feature REGISTRATION_DATE, \n",
    "    # replaced by BUYER_SENIORITY which corresponds to the number of years the buyer is using the service\n",
    "    # ranges from 0 to 16 I believe\n",
    "    df['BUYER_SENIORITY'] = 2017 - df['REGISTRATION_DATE']\n",
    "    seniority = 2017 - train['REGISTRATION_DATE']\n",
    "    crosstab = pd.crosstab(seniority, train.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        BUYER_SENIORITY_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['BUYER_SENIORITY_NOTE_' + str(i)] = df['BUYER_SENIORITY'].map(BUYER_SENIORITY_mapping).fillna(0)\n",
    "        df['BUYER_SENIORITY_NOTE_' + str(i)] = df['BUYER_SENIORITY_NOTE_' + str(i)]*400\n",
    "    del df['BUYER_SENIORITY']\n",
    "    \n",
    "    # Simplification of feature BUYER_BIRTHDAY_DATE, replaced by BUYER_AGE\n",
    "    df['BUYER_AGE_GROUP'] = group_by_age(20, 60, 5, df.BUYER_BIRTHDAY_DATE)\n",
    "    train_copy = train.copy()\n",
    "    train_copy['BUYER_AGE_GROUP'] = group_by_age(20, 60, 5, train_copy.BUYER_BIRTHDAY_DATE)\n",
    "    crosstab = pd.crosstab(train_copy['BUYER_AGE_GROUP'], train_copy.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        BUYER_AGE_GROUP_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['BUYER_AGE_GROUP_NOTE_' + str(i)] = df['BUYER_AGE_GROUP'].map(BUYER_AGE_GROUP_mapping).fillna(0)\n",
    "        df['BUYER_AGE_GROUP_NOTE_' + str(i)] = df['BUYER_AGE_GROUP_NOTE_' + str(i)]*1000\n",
    "    del df['BUYER_AGE_GROUP']\n",
    "    del df['BUYER_BIRTHDAY_DATE']\n",
    "    \n",
    "    # Mapping BUYER_REGION\n",
    "    df['BUYER_REGION'] = df['BUYER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    del df['BUYER_DEPARTMENT']\n",
    "    train_copy = train.copy()\n",
    "    train_copy['BUYER_REGION'] = train_copy['BUYER_DEPARTMENT'].map(DEPARTMENT_mapping)\n",
    "    crosstab = pd.crosstab(train_copy['BUYER_REGION'], train_copy.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        BUYER_REGION_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['BUYER_REGION_NOTE_' + str(i)] = df['BUYER_REGION'].map(BUYER_REGION_mapping).fillna(0)\n",
    "        df['BUYER_REGION_NOTE_' + str(i)] = df['BUYER_REGION_NOTE_' + str(i)]*400\n",
    "    del df['BUYER_REGION']\n",
    "\n",
    "    # Mapping BUYING_DATE\n",
    "    df['BUYING_DATE'] = df['BUYING_DATE'].apply( lambda s : int(re.findall('[0-9]*',s)[0]) )\n",
    "    df['BUYING_DATE'] = df['BUYING_DATE'].fillna(method='ffill')\n",
    "    df['BUYING_DATE'] = df['BUYING_DATE'] * 10\n",
    "\n",
    "    # Mapping SELLER_SCORE_COUNT\n",
    "    SELLER_SCORE_COUNT_mapping = {0.0:0, '<100':1, '100<1000':2, '1000<10000':3, '10000<100000':4, '100000<1000000':5}\n",
    "    categorical_mapping(df, 'SELLER_SCORE_COUNT', SELLER_SCORE_COUNT_mapping)\n",
    "    df['SELLER_SCORE_COUNT'] = df['SELLER_SCORE_COUNT'] * 20\n",
    "\n",
    "    \n",
    "    # Mapping SELLER_SCORE_AVERAGE\n",
    "    df['SELLER_SCORE_AVERAGE'] = df['SELLER_SCORE_AVERAGE'].fillna(value=0)\n",
    "    df['SELLER_SCORE_AVERAGE'] = df['SELLER_SCORE_AVERAGE'] * 2\n",
    "    \n",
    "    # Processing SELLER_COUNTRY variable:\n",
    "    crosstab = pd.crosstab(train['SELLER_COUNTRY'], train.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        SELLER_COUNTRY_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['SELLER_COUNTRY_NOTE_' + str(i)] = df['SELLER_COUNTRY'].map(SELLER_COUNTRY_mapping).fillna(0)\n",
    "        df['SELLER_COUNTRY_NOTE_' + str(i)] = df['SELLER_COUNTRY_NOTE_' + str(i)]*400\n",
    "    del df['SELLER_COUNTRY']\n",
    "    \n",
    "    crosstab = pd.crosstab(train['SELLER_DEPARTMENT'], train.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    # -, DAMAGED, DIFFERENT, FAKE, NOT_RECEIVED, SELLER_CANCEL_POSTERIORI,UNDEFINED, WITHDRAWAL\n",
    "    coeffs = [400, 400, 400, 400, 400, 800, 1000]\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        SELLER_COUNTRY_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['SELLER_DEPARTMENT_NOTE_' + str(i)] = df['SELLER_DEPARTMENT'].map(SELLER_COUNTRY_mapping).fillna(0)\n",
    "        df['SELLER_DEPARTMENT_NOTE_' + str(i)] = df['SELLER_DEPARTMENT_NOTE_' + str(i)] * coeffs[i-1]\n",
    "    del df['SELLER_DEPARTMENT']\n",
    "\n",
    "    # Processing PRODUCT_FAMILY \n",
    "    crosstab = pd.crosstab(train['PRODUCT_FAMILY'], train.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        PRODUCT_FAMILY_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['PRODUCT_FAMILY_NOTE_' + str(i)] = df['PRODUCT_FAMILY'].map(PRODUCT_FAMILY_mapping).fillna(0)\n",
    "        df['PRODUCT_FAMILY_NOTE_' + str(i)] = df['PRODUCT_FAMILY_NOTE_' + str(i)]*400\n",
    "    del df['PRODUCT_FAMILY']\n",
    "\n",
    "    crosstab = pd.crosstab(train['PRODUCT_TYPE'], train.CLAIM_TYPE).sort_values(by=['-'], ascending=False)\n",
    "    for i in range(1, 8):\n",
    "        values = crosstab.values[:,i]\n",
    "        PRODUCT_FAMILY_mapping = values / crosstab.sum(axis = 1)\n",
    "        df['PRODUCT_TYPE_NOTE_' + str(i)] = df['PRODUCT_TYPE'].map(PRODUCT_FAMILY_mapping).fillna(0)\n",
    "        df['PRODUCT_TYPE_NOTE_' + str(i)] = df['PRODUCT_TYPE_NOTE_' + str(i)]*400\n",
    "    del df['PRODUCT_TYPE']\n",
    "\n",
    "    # Mapping ITEM_PRICE:\n",
    "    ITEM_PRICE_mapping = {\"<10\" : 1, \"10<20\" : 2, \"20<50\" : 3, \"50<100\" : 4, \"100<500\" : 5, \"500<1000\" : 6,\n",
    "                          \"1000<5000\" : 7, \">5000\" : 8} \n",
    "    categorical_mapping(df, 'ITEM_PRICE', ITEM_PRICE_mapping)\n",
    "    df['ITEM_PRICE'] = df['ITEM_PRICE'] * 10\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_knn_classifier():\n",
    "    t = train_claims.copy()\n",
    "    data_transformation_claim_type_classifier(t)\n",
    "    tfull = t\n",
    "    t, v = train_test_split(t, test_size = 0.5, random_state=42)\n",
    "    \n",
    "    t_X = t.drop('CLAIM_TYPE',1)\n",
    "    t_Y = t.CLAIM_TYPE\n",
    "    v_X = v.drop('CLAIM_TYPE',1)\n",
    "    v_Y = v.CLAIM_TYPE\n",
    "    \n",
    "    t_Y_multiclass = label_binarize(t_Y, classes=[\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    v_Y_multiclass = label_binarize(v_Y, classes=[\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=1,weights='distance')\n",
    "    knn.fit(t_X, t_Y_multiclass)\n",
    "    s = knn.score(v_X, v_Y_multiclass)\n",
    "    print(\"Score (accuracy) : \", s)\n",
    "    pred_Y = knn.predict(v_X)\n",
    "    print(\"AUC weighted\", auc_weighted(v_Y_multiclass, pred_Y))\n",
    "    return knn\n",
    "    \n",
    "    \n",
    "\n",
    "def build_bagging_classifier():\n",
    "    t = train_claims.copy()\n",
    "    data_transformation_claim_type_classifier(t)\n",
    "    tfull = t\n",
    "    t, v = train_test_split(t, test_size = 0.3, random_state=42)\n",
    "    \n",
    "    t_X = t.drop('CLAIM_TYPE',1)\n",
    "    t_Y = t.CLAIM_TYPE\n",
    "    v_X = v.drop('CLAIM_TYPE',1)\n",
    "    v_Y = v.CLAIM_TYPE\n",
    "    \n",
    "    t_Y_multiclass = label_binarize(t_Y, classes=[\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    v_Y_multiclass = label_binarize(v_Y, classes=[\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=1,weights='distance')\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    cla = BaggingClassifier(knn, n_estimators=50, max_samples=0.1, verbose=1, n_jobs=2) \n",
    "    #cla = BaggingClassifier(knn, n_estimators=100, max_samples=0.1, verbose=1, n_jobs=2)  \n",
    "    cla.fit(t_X, t_Y)\n",
    "    s = cla.score(v_X, v_Y)\n",
    "    print(\"Score (accuracy) : \", s)\n",
    "    pred_Y = cla.predict(v_X)\n",
    "    pred_Y = label_binarize(pred_Y, classes=[\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    print(\"AUC weighted\", auc_weighted(v_Y_multiclass, pred_Y))\n",
    "    return cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = build_knn_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the nearest neighbors\n",
    "index = 42\n",
    "df = train.copy()\n",
    "data_transformation_claim_type_classifier(df)\n",
    "df = df.drop('CLAIM_TYPE', 1)\n",
    "X = df[index:index+1]\n",
    "distances, indexes = knn.kneighbors(X, 15)\n",
    "distances, indexes = distances[0], indexes[0]\n",
    "print(distances)\n",
    "df.iloc[indexes].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = build_bagging_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BAggingClassifier is really slow but predicts correctly the claim type nearly 40% of the time.\n",
    "Considering that there are seven classes, this is not bad at all.\n",
    "In my lastest experiment, I got the same performances using a Random Forest instead of the BaggingClassifier. The RandomForest is clearly faster. Thus, Fabien is probably right to choose RandomForest as its default choice !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_classifier(claim_predictor, claim_type_predictor):\n",
    "    v = valid.copy()\n",
    "    data_transformation_claim_predictor(v)\n",
    "    v = v.drop('CLAIM',1)\n",
    "    if type(claim_predictor).__name__ == 'Sequential':\n",
    "        v = v.values\n",
    "    claim_or_no = claim_predictor.predict(v)\n",
    "    if len(claim_or_no.shape) == 2: # when the predictor is the Keras deep network\n",
    "        claim_or_no = claim_or_no[:,1]\n",
    "    \n",
    "    v = valid.copy()\n",
    "    data_transformation_claim_type_classifier(v)\n",
    "    v = v.drop('CLAIM_TYPE',1)\n",
    "    claim_type = claim_type_predictor.predict(v)\n",
    "    if type(claim_type_predictor).__name__ == 'BaggingClassifier':\n",
    "        claim_type = label_binarize(claim_type, classes=[\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    \n",
    "    preds = []\n",
    "    no_claim_Y = np.zeros(8); no_claim_Y[0] = 1\n",
    "    for i in range(len(claim_or_no)):\n",
    "        if claim_or_no[i] > 0.5:\n",
    "            preds.append(np.concatenate(([0], claim_type[i])))\n",
    "        else:\n",
    "            preds.append(no_claim_Y)\n",
    "    v_Y = valid.copy().CLAIM_TYPE\n",
    "    v_Y_multiclass = label_binarize(v_Y, classes=[\"-\", \"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    s = auc_weighted(v_Y_multiclass, preds)\n",
    "    print(\"AUC weighted :\", s)\n",
    "    \n",
    "    def claim_type_from_categorical(claims):\n",
    "        classes = [\"-\",\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\"SELLER_CANCEL_POSTERIORI\",\n",
    "             \"NOT_RECEIVED\", \"UNDEFINED\",\"FAKE\"]\n",
    "        def claim_type(p):\n",
    "            return classes[np.argmax(p)]\n",
    "        return np.array([claim_type(c) for c in claims])\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    preds = claim_type_from_categorical(preds)\n",
    "    return confusion_matrix(v_Y, preds), v_Y, preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, v_Y, pred_Y = eval_classifier(rf, knn)\n",
    "print(cm)\n",
    "pd.crosstab(v_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classifier(deep, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classifier(rf, bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classifier(deep, bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_on_test(claim_predictor, claim_type_predictor):\n",
    "    t = test_X.copy()\n",
    "    data_transformation_claim_predictor(t)\n",
    "    if type(claim_predictor).__name__ == 'Sequential':\n",
    "        t = t.values\n",
    "    claim_or_no = claim_predictor.predict(t)\n",
    "    if len(claim_or_no.shape) == 2: # when the predictor is the Keras deep network\n",
    "        claim_or_no = claim_or_no[:,1]\n",
    "    \n",
    "    t = test_X.copy()\n",
    "    data_transformation_claim_type_classifier(t)\n",
    "    claim_type = claim_type_predictor.predict(t)\n",
    "    if type(claim_type_predictor).__name__ == 'BaggingClassifier':\n",
    "        claim_type = label_binarize(claim_type, classes=[\"WITHDRAWAL\",\"DAMAGED\",\"DIFFERENT\",\n",
    "                                                           \"SELLER_CANCEL_POSTERIORI\",\"NOT_RECEIVED\",\n",
    "                                                           \"UNDEFINED\",\"FAKE\"])\n",
    "    \n",
    "    preds = []\n",
    "    no_claim_Y = np.zeros(8); no_claim_Y[0] = 1\n",
    "    for i in range(len(claim_or_no)):\n",
    "        if claim_or_no[i] > 0.5:\n",
    "            preds.append(np.concatenate(([0], claim_type[i])))\n",
    "        else:\n",
    "            preds.append(no_claim_Y)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = prediction_on_test(rf, bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_claims_prediction(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
